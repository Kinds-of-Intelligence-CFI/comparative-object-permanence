{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kinds-of-Intelligence-CFI/comparative-object-permanence/blob/multivariate-measurement-layout/analysis/measurement-layouts/object_permanence_measurement_layout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparative Object Permanence Measurement Layouts\n",
        "\n",
        "Authors: K. Voudouris, J. Burden, J. Hernández-Orallo, M. Tešić"
      ],
      "metadata": {
        "id": "U27yVRKBqCpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INIT"
      ],
      "metadata": {
        "id": "95RSQ17Lriav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLYI9s6Lp3fi"
      },
      "outputs": [],
      "source": [
        "!pip install pymc --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip install arviz --quiet\n",
        "!pip install erroranalysis --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import erroranalysis as ea\n",
        "import gc\n",
        "import graphviz\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pymc as pm\n",
        "import random as rm\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "from pymc import model\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\")"
      ],
      "metadata": {
        "id": "ehpymOwzrgm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ga3CMVsXSYRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data\n"
      ],
      "metadata": {
        "id": "s5Nsn4LgzVmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agents_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/multivariate-measurement-layout/analysis/measurement-layouts/results_final_clean_agents_wide.csv'\n",
        "agent_data = pd.read_csv(agents_url)\n",
        "\n",
        "children_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/multivariate-measurement-layout/analysis/measurement-layouts/results_final_clean_children_wide.csv'\n",
        "children_data = pd.read_csv(children_url)\n",
        "\n",
        "synthetic_agents_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/multivariate-measurement-layout/analysis/measurement-layouts/results_synthetic_agents_wide.csv'\n",
        "synthetic_agents_data = pd.read_csv(synthetic_agents_url)\n",
        "\n",
        "long_data_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/multivariate-measurement-layout/analysis/results_final_clean_long.csv'\n",
        "long_data = pd.read_csv(long_data_url)"
      ],
      "metadata": {
        "id": "1dN0OHAWzVBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Some Useful Functions"
      ],
      "metadata": {
        "id": "1s2i0egJA6Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(x, L=1, k=1, x_0=0):\n",
        "  return (L / (1 + np.exp(-k * (x-x_0))))\n",
        "\n",
        "def logistic999(x, min, max):    # This logistic function ensures that if x is at -(max-min), we get prob 0.001, and if x is at (max-min), we get prob 0.999\n",
        "  x = x - min\n",
        "  max = max - min\n",
        "  x = 6.90675478 * x / max\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def visualAcuityLOMargin(ability, goalSize, goalDistance): # must return a value between -inf and inf  (more precisely between -maxVisualAcuityAbility and maxVisualAcuityAbility)\n",
        "  margin = np.log(ability)-np.log((goalDistance/goalSize))\n",
        "\n",
        "  return margin   # Goes between -inf to inf, with logodds=0 meaning this would lead to 0.5 chance of success\n",
        "\n",
        "def SimplePrMargin(ability, binaryFeature): # must return a value between 0 and 1\n",
        "  return 1-((1-ability)*binaryFeature)  # If binaryFeature is 0 then the margin represents p(success)=1. If binaryFeature = 1 then p(success)=ability\n",
        "\n",
        "def flatNavMargin(ability, distanceToGoal, numTurns):\n",
        "  return ability - ((distanceToGoal * numTurns))\n",
        "\n",
        "def scaledBeta(name, a, b, min, max, simpleGraph = True):\n",
        "  if (simpleGraph):\n",
        "    beta = pm.Beta(name, a, b)\n",
        "    return beta * (max - min) + min\n",
        "  else:\n",
        "    beta = pm.Beta(f\"{name}_raw\", a, b)\n",
        "    return pm.Deterministic(name, beta * (max - min) + min)\n"
      ],
      "metadata": {
        "id": "-uobQpcJiPr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function for pulling out the means and standard deviations for abilities of interest\n",
        "def analyzeAgentResults(agentData, abilitiesToShow):\n",
        "\n",
        "  abilityMeans = {}\n",
        "  abilitySDs = {}\n",
        "\n",
        "  for a in abilitiesToShow: #iterate through each ability, add posterior mean to dataframe, and plot posterior\n",
        "\n",
        "    posteriorMean = float(np.mean(agentData['posterior'][a])) # calculate posterior mean\n",
        "    posteriorSD = float(np.std(agentData['posterior'][a])) #calculate posterior sd\n",
        "    abilityMeans[a] = posteriorMean\n",
        "    abilitySDs[a] = posteriorSD\n",
        "\n",
        "  return abilityMeans, abilitySDs\n",
        "\n",
        "# A function for padding the testing data with 0s for prediction\n",
        "def pad(testingData, trainingDataSize):\n",
        "    return testingData.append(pd.Series(np.zeros(trainingDataSize-len(testingData), dtype=int)))\n",
        "\n",
        "# A function for making predictions based on a fitted measurement layout\n",
        "def predict(m, agentData, dfTest, agent_success, agent_choice, len_training, multi_var = True):\n",
        "  with m:\n",
        "    # set the data for prediction\n",
        "    pm.set_data({\"lavaPresence\": pad(dfTest[\"lavaPresence\"], len_training)})\n",
        "    pm.set_data({\"numChoices\": pad(dfTest[\"numChoices\"], len_training)})\n",
        "    pm.set_data({\"minTurnsToGoal\": pad(dfTest[\"minNumTurnsGoal\"], len_training)})\n",
        "    pm.set_data({\"goalDistance\": pad(dfTest[\"minDistToGoal\"], len_training)})\n",
        "    if multi_var:\n",
        "      pm.set_data({\"minTurnsToCorrectChoice\": pad(dfTest[\"minNumTurnsChoice\"], len_training)})\n",
        "      pm.set_data({\"choiceDistance\": pad(dfTest[\"minDistToCorrectChoice\"], len_training)})\n",
        "    pm.set_data({\"goalSize\": pad(dfTest[\"mainGoalSize\"], len_training)})\n",
        "    pm.set_data({\"goalRight\": pad(dfTest[\"goalRightRelToStart\"], len_training)})\n",
        "    pm.set_data({\"goalAhead\": pad(dfTest[\"goalCentreRelToStart\"], len_training)})\n",
        "    pm.set_data({\"goalLeft\": pad(dfTest[\"goalLeftRelToStart\"], len_training)})\n",
        "    pm.set_data({\"allocentricOPTest\": pad(dfTest[\"goalBecomesAllocentricallyOccluded\"], len_training)})\n",
        "\n",
        "    if multi_var:\n",
        "      predictions = pm.sample_posterior_predictive(agentData, var_names=[\"choiceP\", \"successP\"], return_inferencedata=False, predictions=True, extend_inferencedata=False)\n",
        "      predictionSuccessChainRuns = predictions[\"successP\"][:,:,0:len(dfTest)]\n",
        "      predictionsSuccessInstance = np.mean(predictionSuccessChainRuns, (0,1))\n",
        "      predictionChoiceChainRuns = predictions[\"choiceP\"][:,:,0:len(dfTest)]\n",
        "      predictionsChoiceInstance = np.mean(predictionChoiceChainRuns, (0,1))\n",
        "\n",
        "      successes = dfTest[agent_success].to_numpy()\n",
        "      choices = dfTest[agent_choice].to_numpy()\n",
        "\n",
        "      return predictionsSuccessInstance, predictionsChoiceInstance, successes, choices\n",
        "\n",
        "    else:\n",
        "      predictions=pm.sample_posterior_predictive(agentData, var_names=[\"finalP\"], return_inferencedata=False,predictions=True,extend_inferencedata=False)\n",
        "      predictionChainRuns =predictions[\"finalP\"][:,:,0:len(dfTest)]\n",
        "      predictionsInstance = np.mean(predictionChainRuns, (0,1))\n",
        "\n",
        "      return predictionsInstance,  dfTest[agent].to_numpy()\n",
        "\n",
        "\n",
        "def brierScore(preds, outs):\n",
        "    return 1/len(preds) * sum( (preds-outs)**2 )\n",
        "\n",
        "def brierDecomp(preds, outs):\n",
        "\n",
        "  brier= 1/len(preds) * sum( (preds-outs)**2 )\n",
        "  ## bin predictions\n",
        "  bins = np.linspace(0,1,11)\n",
        "  binCenters = (bins[:-1] +bins[1:]) /2\n",
        "  binPredInds = np.digitize(preds,binCenters)\n",
        "  binnedPreds = bins[binPredInds]\n",
        "\n",
        "  binTrueFreqs = np.zeros(10)\n",
        "  binPredFreqs = np.zeros(10)\n",
        "  binCounts = np.zeros(10)\n",
        "\n",
        "  for i in range(10):\n",
        "      idx = (preds >= bins[i]) & (preds < bins[i+1])\n",
        "\n",
        "      binTrueFreqs[i] = np.sum(outs[idx])/np.sum(idx) if np.sum(idx) > 0 else 0\n",
        "     # print(np.sum(outs[idx]), np.sum(idx), binTrueFreqs[i])\n",
        "      binPredFreqs[i] = np.mean(preds[idx]) if np.sum(idx) > 0 else 0\n",
        "      binCounts[i] = np.sum(idx)\n",
        "\n",
        "  calibration = np.sum(binCounts * (binTrueFreqs - binPredFreqs) ** 2) / np.sum(binCounts) if np.sum(binCounts) > 0 else 0\n",
        "  refinement = np.sum(binCounts * (binTrueFreqs *(1 - binTrueFreqs))) / np.sum(binCounts) if np.sum(binCounts) > 0 else 0\n",
        "  # Compute refinement component\n",
        "  #refinement = brier - calibration\n",
        "  return brier, calibration, refinement\n",
        "\n"
      ],
      "metadata": {
        "id": "-Rm-t0AkyvM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Pdw0MZkvya3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Measurement Layout"
      ],
      "metadata": {
        "id": "C0_GsfhIYZPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Modelling\n",
        "\n",
        "Measurement layouts with just success as the dependent variable."
      ],
      "metadata": {
        "id": "HNklql-Eg6CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SetupUnivariateBasicModel(taskResultsAll, uniformAbilitySlack, agent_type, agent_name = None, sample = 500):\n",
        "  \"\"\"\n",
        "  taskResults is the conjunction of the metadata with the successes of the agents on that set of tests.\n",
        "  \"\"\"\n",
        "  assert uniformAbilitySlack >=1, \"Slack must be greater than or equal to 1.\"\n",
        "\n",
        "  if agent_type == \"agent_real\" or agent_type == \"agent_synthetic\":\n",
        "    taskResults = taskResultsAll.dropna(subset = [agent_name])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults[agent_name]\n",
        "  elif agent_type == \"child\":\n",
        "    taskResults = taskResultsAll.dropna(subset = ['success'])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults['success']\n",
        "  else:\n",
        "    print(\"Agent not recognised. Quitting.\")\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  abilityMin = {} #Initialize ability Min and max dataframes for plotting ranges\n",
        "  abilityMax = {}\n",
        "\n",
        "  #Decide \"maximum capabilities\" based on the hardest values in the dataset\n",
        "  maxDistance = taskResults[\"minDistToGoal\"].max()\n",
        "  maxTurns = taskResults[\"minNumTurnsGoal\"].max()\n",
        "  maxChoices = taskResults[\"numChoices\"].max()\n",
        "  maxGoalSize = taskResults[\"mainGoalSize\"].max() * uniformAbilitySlack\n",
        "  maxPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).max()) * uniformAbilitySlack\n",
        "  maxFlatNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).max() * uniformAbilitySlack\n",
        "\n",
        "  # Decide 'minimum capabilities' based on easiest values in the dataset\n",
        "  minDistance = taskResults[\"minDistToGoal\"].min()\n",
        "  minTurns = taskResults[\"minNumTurnsGoal\"].min()\n",
        "  minChoices = taskResults[\"numChoices\"].min()\n",
        "  minGoalSize = taskResults[\"mainGoalSize\"].min() * (1-(uniformAbilitySlack-1))\n",
        "  minPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).min()) * (1-(uniformAbilitySlack-1))\n",
        "  minFlatNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).min() * (1-(uniformAbilitySlack-1))\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # OP Ability\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "    abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "    abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "    # Visual acuity\n",
        "    visualAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minDistance/maxGoalSize, maxDistance/minGoalSize)\n",
        "    abilityMin[\"visualAcuityAbility\"] = minDistance/maxGoalSize\n",
        "    abilityMax[\"visualAcuityAbility\"] = maxDistance/minGoalSize\n",
        "\n",
        "    # Flat Navigation Ability\n",
        "    flatNavAbility = pm.Uniform(\"flatNavAbility\", minFlatNav, maxFlatNav)      # how much navigation is involved, i.e, how far away and how circuitous is the path to the goal?\n",
        "    abilityMin[\"flatNavAbility\"] = minFlatNav\n",
        "    abilityMax[\"flatNavAbility\"] = maxFlatNav\n",
        "\n",
        "    # Lava Ability\n",
        "    lavaAbility = scaledBeta(\"lavaAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"lavaAbility\"] = 0\n",
        "    abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "    # Goal Right Ability\n",
        "    rightAbility = scaledBeta(\"rightAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"rightAbility\"] = 0\n",
        "    abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "    # Goal Left Ability\n",
        "    leftAbility = scaledBeta(\"leftAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"leftAbility\"] = 0\n",
        "    abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "    # Goal Ahead Ability\n",
        "    aheadAbility = scaledBeta(\"aheadAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"aheadAbility\"] = 0\n",
        "    abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "    ## Environment variables as Deterministic (about the instance)\n",
        "\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", taskResults[\"lavaPresence\"].values)\n",
        "    numGoals = pm.MutableData(\"numberOfGoals\", taskResults[\"numGoalsAll\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", taskResults[\"numChoices\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\",taskResults[\"mainGoalSize\"].values)\n",
        "    goalDist = pm.MutableData(\"goalDistance\", taskResults[\"minDistToGoal\"].values)\n",
        "    numTurns = pm.MutableData(\"minTurnsToGoal\", taskResults[\"minNumTurnsGoal\"])\n",
        "    goalRight = pm.MutableData(\"goalRight\", taskResults[\"goalRightRelToStart\"])\n",
        "    goalAhead = pm.MutableData(\"goalAhead\", taskResults[\"goalCentreRelToStart\"])\n",
        "    goalLeft = pm.MutableData(\"goalLeft\", taskResults[\"goalLeftRelToStart\"])\n",
        "    opTest = pm.MutableData(\"allocentricOPTest\", taskResults[\"goalBecomesAllocentricallyOccluded\"].values)\n",
        "\n",
        "    ## Margins\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic(visualAcuityLOMargin(visualAcuityAbility, goalSize, goalDist)))\n",
        "\n",
        "    rightP = pm.Deterministic(\"rightPerformance\", logistic999(SimplePrMargin(rightAbility, goalRight), min = 0, max = 1))\n",
        "    aheadP = pm.Deterministic(\"aheadPerformance\", logistic999(SimplePrMargin(aheadAbility, goalAhead), min = 0, max = 1))\n",
        "    leftP = pm.Deterministic(\"leftPerformance\", logistic999(SimplePrMargin(leftAbility, goalLeft), min = 0, max = 1))\n",
        "\n",
        "    flatNavP = pm.Deterministic(\"flatNavP\", logistic999(flatNavAbility - (numTurns * goalDist), min = minFlatNav, max = maxFlatNav))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(SimplePrMargin(lavaAbility, lavaPresence), min = 0, max = 1))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", (flatNavP * lavaP * rightP * aheadP * leftP))\n",
        "\n",
        "    OPPerformance = (objPermAbility  - (((goalDist*opTest) * numChoices) + (numChoices * (1-opTest))))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", (logistic999(OPPerformance, min = minPermAbility, max = maxPermAbility)))\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP * visualAcuityP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed=results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "S5qjDlzpg5vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = SetupUnivariateBasicModel(agent_data, uniformAbilitySlack=1, agent_type='agent_real', agent_name=\"success_ppo_bc_all_2023\", sample = None)\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv.format = \"png\"\n",
        "gv.render(filename='viz/UnivariateNonHierarchicalMeasurementLayout.gv', directory='/content/drive/Shareddrives/comparative-object-permanence')\n",
        "Image(\"/content/drive/Shareddrives/comparative-object-permanence/UnivariateNonHierarchicalMeasurementLayout.gv.png\")\n",
        "gv"
      ],
      "metadata": {
        "id": "DnzQ17_Nh10m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SetupUnivariateHierarchicalModel(taskResults, agent_col, uniformAbilitySlack = 1):\n",
        "  \"\"\"\n",
        "  taskResults is the conjunction of the metadata with the successes of the agents on that set of tests. Needs to be in long format\n",
        "  \"\"\"\n",
        "  assert uniformAbilitySlack >=1, \"Slack must be greater than or equal to 1.\"\n",
        "\n",
        "  agent_idx, agents = taskResults[agent_col].factorize(sort=True)\n",
        "\n",
        "  COORDS = {\n",
        "    'obs': taskResults.index,\n",
        "    'agent': agents ##only by type of agent, not by random seed.\n",
        "    }\n",
        "\n",
        "  abilityMin = {} #Initialize ability Min and max dataframes for plotting ranges\n",
        "  abilityMax = {}\n",
        "\n",
        "  #Decide \"maximum capabilities\" based on the hardest values in the dataset\n",
        "  maxDistance = taskResults[\"minDistToGoal\"].max()\n",
        "  maxTurns = taskResults[\"minNumTurnsGoal\"].max()\n",
        "  maxChoices = taskResults[\"numChoices\"].max()\n",
        "  maxGoalSize = taskResults[\"mainGoalSize\"].max() * uniformAbilitySlack\n",
        "  maxPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).max()) * uniformAbilitySlack\n",
        "  maxFlatNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).max() * uniformAbilitySlack\n",
        "\n",
        "  # Decide 'minimum capabilities' based on easiest values in the dataset\n",
        "  minDistance = taskResults[\"minDistToGoal\"].min()\n",
        "  minTurns = taskResults[\"minNumTurnsGoal\"].min()\n",
        "  minChoices = taskResults[\"numChoices\"].min()\n",
        "  minGoalSize = taskResults[\"mainGoalSize\"].min() * (1-(uniformAbilitySlack-1))\n",
        "  minPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).min()) * (1-(uniformAbilitySlack-1))\n",
        "  minFlatNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).min() * (1-(uniformAbilitySlack-1))\n",
        "\n",
        "  m = pm.Model(coords=COORDS)\n",
        "  with m:\n",
        "\n",
        "    #### Hyper priors ####\n",
        "    V_Lava = pm.Beta('V_Lava', 1, 1)\n",
        "    U_Lava = pm.HalfNormal('U_Lava',sigma = 10)\n",
        "    V_Ahead = pm.Beta('V_Ahead', 1, 1)\n",
        "    U_Ahead = pm.HalfNormal('U_Ahead',sigma = 10)\n",
        "    V_Left = pm.Beta('V_Left', 1, 1)\n",
        "    U_Left = pm.HalfNormal('U_Left',sigma = 10)\n",
        "    V_Right = pm.Beta('V_Right', 1, 1)\n",
        "    U_Right = pm.HalfNormal('U_Right',sigma = 10)\n",
        "\n",
        "    V_OP = pm.Beta('V_OP', 1, 1)\n",
        "    U_OP = pm.HalfNormal('U_OP',sigma = 10)\n",
        "    V_VisualAcuity = pm.Beta('V_VisualAcuity', 1, 1)\n",
        "    U_VisualAcuity = pm.HalfNormal('U_VisualAcuity',sigma = 10)\n",
        "    V_FlatNav = pm.Beta('V_FlatNav', 1, 1)\n",
        "    U_FlatNav = pm.HalfNormal('U_FlatNav',sigma = 10)\n",
        "\n",
        "    ##ignore noise for now.\n",
        "\n",
        "    ### Ability Priors as scaled beta distributions\n",
        "    al_Lava = pm.Deterministic('al_Lava', V_Lava * U_Lava)\n",
        "    be_Lava = pm.Deterministic('be_Lava', (1-V_Lava) * U_Lava)\n",
        "    al_Ahead = pm.Deterministic('al_Ahead', V_Ahead * U_Ahead)\n",
        "    be_Ahead = pm.Deterministic('be_Ahead', (1-V_Ahead) * U_Ahead)\n",
        "    al_Left = pm.Deterministic('al_Left', V_Left * U_Left)\n",
        "    be_Left = pm.Deterministic('be_Left', (1-V_Left) * U_Left)\n",
        "    al_Right = pm.Deterministic('al_Right', V_Right * U_Right)\n",
        "    be_Right = pm.Deterministic('be_Right', (1-V_Right) * U_Right)\n",
        "\n",
        "    al_OP = pm.Deterministic('al_OP', V_OP * U_OP)\n",
        "    be_OP = pm.Deterministic('be_OP', (1-V_OP) * U_OP)\n",
        "    al_VisualAcuity = pm.Deterministic('al_VisualAcuity', V_VisualAcuity * U_VisualAcuity)\n",
        "    be_VisualAcuity = pm.Deterministic('be_VisualAcuity', (1-V_VisualAcuity) * U_VisualAcuity)\n",
        "    al_FlatNav = pm.Deterministic('al_FlatNav', V_FlatNav * U_FlatNav)\n",
        "    be_FlatNav = pm.Deterministic('be_FlatNav', (1-V_FlatNav) * U_FlatNav)\n",
        "\n",
        "    # Parametrise the beta distributions\n",
        "    z1_Lava = pm.Gamma(\"z1_Lava\", alpha=al_Lava, beta=1, dims = 'agent')\n",
        "    z2_Lava = pm.Gamma(\"z2_Lava\", alpha=be_Lava, beta=1, dims = 'agent')\n",
        "    z1_Ahead = pm.Gamma(\"z1_Ahead\", alpha=al_Ahead, beta=1, dims = 'agent')\n",
        "    z2_Ahead = pm.Gamma(\"z2_Ahead\", alpha=be_Ahead, beta=1, dims = 'agent')\n",
        "    z1_Left = pm.Gamma(\"z1_Left\", alpha=al_Left, beta=1, dims = 'agent')\n",
        "    z2_Left = pm.Gamma(\"z2_Left\", alpha=be_Left, beta=1, dims = 'agent')\n",
        "    z1_Right = pm.Gamma(\"z1_Right\", alpha=al_Right, beta=1, dims = 'agent')\n",
        "    z2_Right = pm.Gamma(\"z2_Right\", alpha=be_Right, beta=1, dims = 'agent')\n",
        "\n",
        "    z1_OP = pm.Gamma(\"z1_OP\", alpha=al_OP, beta=1, dims = 'agent')\n",
        "    z2_OP = pm.Gamma(\"z2_OP\", alpha=be_OP, beta=1, dims = 'agent')\n",
        "    z1_VisualAcuity = pm.Gamma(\"z1_VisualAcuity\", alpha=al_VisualAcuity, beta=1, dims = 'agent')\n",
        "    z2_VisualAcuity = pm.Gamma(\"z2_VisualAcuity\", alpha=be_VisualAcuity, beta=1, dims = 'agent')\n",
        "    z1_FlatNav = pm.Gamma(\"z1_FlatNav\", alpha=al_FlatNav, beta=1, dims = 'agent')\n",
        "    z2_FlatNav = pm.Gamma(\"z2_FlatNav\", alpha=be_FlatNav, beta=1, dims = 'agent')\n",
        "\n",
        "    ## Abilities\n",
        "\n",
        "    lavaAbility = pm.Deterministic('lavaAbility',(z1_Lava/(z1_Lava+z2_Lava)),dims='agent')\n",
        "    abilityMin[\"lavaAbility\"] = 0\n",
        "    abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "    aheadAbility = pm.Deterministic('aheadAbility',(z1_Ahead/(z1_Ahead+z2_Ahead)),dims='agent')\n",
        "    abilityMin[\"aheadAbility\"] = 0\n",
        "    abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "    leftAbility = pm.Deterministic('leftAbility',(z1_Left/(z1_Left+z2_Left)),dims='agent')\n",
        "    abilityMin[\"leftAbility\"] = 0\n",
        "    abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "    rightAbility = pm.Deterministic('rightAbility',(z1_Right/(z1_Right+z2_Right)),dims='agent')\n",
        "    abilityMin[\"rightAbility\"] = 0\n",
        "    abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "    objPermAbility = pm.Deterministic(\"objPermAbility\", ((z1_OP/(z1_OP+z2_OP))*maxPermAbility)+minPermAbility, dims='agent')  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "    abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "    abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "    abilityMin[\"visualAcuityAbility\"] = minDistance/maxGoalSize\n",
        "    abilityMax[\"visualAcuityAbility\"] = maxDistance/minGoalSize\n",
        "    visualAcuityAbility = pm.Deterministic(\"visualAcuityAbility\", ((z1_VisualAcuity/(z1_VisualAcuity+z2_VisualAcuity))*abilityMax[\"visualAcuityAbility\"])+abilityMin[\"visualAcuityAbility\"], dims='agent')  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "\n",
        "    abilityMin[\"flatNavAbility\"] = minFlatNav\n",
        "    abilityMax[\"flatNavAbility\"] = maxFlatNav\n",
        "    flatNavAbility = pm.Deterministic(\"flatNavAbility\", ((z1_FlatNav/(z1_FlatNav+z2_FlatNav))*maxFlatNav)+minFlatNav, dims='agent')  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "\n",
        "    ## Environment variables as Deterministic (about the instance)\n",
        "\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", taskResults[\"lavaPresence\"].values, dims='obs')\n",
        "    numGoals = pm.MutableData(\"numberOfGoals\", taskResults[\"numGoalsAll\"].values, dims='obs')\n",
        "    numChoices = pm.MutableData(\"numChoices\", taskResults[\"numChoices\"].values, dims='obs')\n",
        "    goalSize = pm.MutableData(\"goalSize\",taskResults[\"mainGoalSize\"].values, dims='obs')\n",
        "    goalDist = pm.MutableData(\"goalDistance\", taskResults[\"minDistToGoal\"].values, dims='obs')\n",
        "    numTurns = pm.MutableData(\"minTurnsToGoal\", taskResults[\"minNumTurnsGoal\"], dims='obs')\n",
        "    goalRight = pm.MutableData(\"goalRight\", taskResults[\"goalRightRelToStart\"], dims='obs')\n",
        "    goalAhead = pm.MutableData(\"goalAhead\", taskResults[\"goalCentreRelToStart\"], dims='obs')\n",
        "    goalLeft = pm.MutableData(\"goalLeft\", taskResults[\"goalLeftRelToStart\"], dims='obs')\n",
        "    opTest = pm.MutableData(\"allocentricOPTest\", taskResults[\"goalBecomesAllocentricallyOccluded\"].values, dims='obs')\n",
        "\n",
        "    ## Margins\n",
        "\n",
        "    rightP = pm.Deterministic(\"rightPerformance\", logistic999(SimplePrMargin(rightAbility[agent_idx], goalRight), min = 0, max = 1),dims='obs')\n",
        "    aheadP = pm.Deterministic(\"aheadPerformance\", logistic999(SimplePrMargin(aheadAbility[agent_idx], goalAhead), min = 0, max = 1),dims='obs')\n",
        "    leftP = pm.Deterministic(\"leftPerformance\", logistic999(SimplePrMargin(leftAbility[agent_idx], goalLeft), min = 0, max = 1),dims='obs')\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic(visualAcuityLOMargin(visualAcuityAbility[agent_idx], goalSize, goalDist)),dims='obs')\n",
        "\n",
        "    flatNavP = pm.Deterministic(\"flatNavP\", logistic999(flatNavAbility[agent_idx] - (numTurns * goalDist), min = minFlatNav, max = maxFlatNav),dims='obs')\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(SimplePrMargin(lavaAbility[agent_idx], lavaPresence), min = 0, max = 1),dims='obs')\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", (flatNavP * lavaP * rightP * aheadP * leftP),dims='obs')\n",
        "\n",
        "    OPPerformance = pm.Deterministic(\"OPPerf\", objPermAbility[agent_idx]  - (((goalDist*opTest) * numChoices) + (numChoices * (1-opTest))), dims='obs')\n",
        "\n",
        "    objPermP = pm.Deterministic(\"objPermP\", (logistic999(OPPerformance, min = minPermAbility, max = maxPermAbility)),dims='obs')\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP * visualAcuityP),dims='obs')\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed=taskResults['success'].values,dims='obs')\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "q8OUQOWrluJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomWalkers = long_data[(long_data['agent_tag_seed'].str.contains(\"Random\"))]\n",
        "hierarchical_model, abilityMins, abilityMaxs = SetupUnivariateHierarchicalModel(randomWalkers, agent_col = 'agent_tag_seed')\n",
        "pm.model_to_graphviz(hierarchical_model)\n",
        "gv.format = \"png\"\n",
        "gv.render(filename='viz/UnivariateHierarchicalMeasurementLayout.gv', directory='/content/drive/Shareddrives/comparative-object-permanence')\n",
        "Image(\"/content/drive/Shareddrives/comparative-object-permanence/UnivariateHierarchicalMeasurementLayout.gv.png\")\n",
        "gv"
      ],
      "metadata": {
        "id": "pdIDpZXKm-bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate Modelling\n",
        "\n",
        "Measurement layouts with success and task choice as dependent variable, in an effort to stabilise outputs for object permanence and navigation."
      ],
      "metadata": {
        "id": "IidCg6WSg2YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SetupMultivariateBasicModel(taskResultsAll, uniformAbilitySlack, agent_type, agent_name_success = None, agent_name_choice = None, sample = 500):\n",
        "  \"\"\"\n",
        "  taskResults is the conjunction of the metadata with the successes of the agents on that set of tests.\n",
        "  \"\"\"\n",
        "  assert uniformAbilitySlack >=1, \"Slack must be greater than or equal to 1.\"\n",
        "\n",
        "  if agent_type == \"agent_real\" or agent_type == \"agent_synthetic\":\n",
        "    taskResults = taskResultsAll.dropna(subset = [agent_name_success, agent_name_choice])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults[agent_name_success]\n",
        "    choices = taskResults[agent_name_choice]\n",
        "  elif agent_type == \"child\":\n",
        "    taskResults = taskResultsAll.dropna(subset = ['success', 'correctChoice'])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults['success']\n",
        "    choices = taskResults['correctChoice']\n",
        "  else:\n",
        "    print(\"Agent not recognised. Quitting.\")\n",
        "    return\n",
        "\n",
        "  abilityMin = {} #Initialize ability Min and max dataframes for plotting ranges\n",
        "  abilityMax = {}\n",
        "\n",
        "  #Decide \"maximum capabilities\" based on the hardest values in the dataset\n",
        "  maxDistanceGoal = taskResults[\"minDistToGoal\"].max()\n",
        "  maxTurnsGoal = taskResults[\"minNumTurnsGoal\"].max()\n",
        "  maxChoices = taskResults[\"numChoices\"].max()\n",
        "  maxGoalSize = taskResults[\"mainGoalSize\"].max() * uniformAbilitySlack\n",
        "  maxPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).max()) * uniformAbilitySlack\n",
        "  maxDistanceChoice = taskResults[\"minDistToCorrectChoice\"].max()\n",
        "  maxTurnsChoice = taskResults[\"minNumTurnsChoice\"].max()\n",
        "\n",
        "  maxChoiceNav = ((taskResults[\"minDistToCorrectChoice\"]*taskResults[\"minNumTurnsChoice\"])).max() * uniformAbilitySlack # instrumental, not used in measurement layout explicitly\n",
        "  maxSuccessNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).max() * uniformAbilitySlack # instrumental, not used in measurement layout explicitly\n",
        "\n",
        "  maxFlatNav = max([maxChoiceNav, maxSuccessNav]) # pick the largest of the two products\n",
        "\n",
        "\n",
        "  # Decide 'minimum capabilities' based on easiest values in the dataset\n",
        "  minDistanceGoal = taskResults[\"minDistToGoal\"].min()\n",
        "  minTurnsGoal = taskResults[\"minNumTurnsGoal\"].min()\n",
        "  minChoices = taskResults[\"numChoices\"].min()\n",
        "  minGoalSize = taskResults[\"mainGoalSize\"].min() * (1-(uniformAbilitySlack-1))\n",
        "  minPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).min()) * (1-(uniformAbilitySlack-1))\n",
        "  minDistanceChoice = taskResults[\"minDistToCorrectChoice\"].min()\n",
        "  minTurnsChoice = taskResults[\"minNumTurnsChoice\"].min()\n",
        "\n",
        "  minChoiceNav = ((taskResults[\"minDistToCorrectChoice\"]*taskResults[\"minNumTurnsChoice\"])).min() * (1-(uniformAbilitySlack-1)) # instrumental, not used in measurement layout explicitly\n",
        "  minSuccessNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).min() * (1-(uniformAbilitySlack-1)) # instrumental, not used in measurement layout explicitly\n",
        "\n",
        "  minFlatNav = min([minChoiceNav, minSuccessNav])\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "    abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "    abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "    #Visual acuity\n",
        "    visualAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minDistanceGoal/maxGoalSize, maxDistanceGoal/minGoalSize)\n",
        "    abilityMin[\"visualAcuityAbility\"] = minDistanceGoal/maxGoalSize\n",
        "    abilityMax[\"visualAcuityAbility\"] = maxDistanceGoal/minGoalSize\n",
        "\n",
        "    # Flat Navigation Ability\n",
        "    flatNavAbility = pm.Uniform(\"flatNavAbility\", minFlatNav, maxFlatNav)\n",
        "    abilityMin[\"flatNavAbility\"] = minFlatNav\n",
        "    abilityMax[\"flatNavAbility\"] = maxFlatNav\n",
        "\n",
        "    # Lava Ability\n",
        "    lavaAbility = scaledBeta(\"lavaAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"lavaAbility\"] = 0\n",
        "    abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "    # Goal Right Ability\n",
        "    rightAbility = scaledBeta(\"rightAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"rightAbility\"] = 0\n",
        "    abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "    # Goal Left Ability\n",
        "    leftAbility = scaledBeta(\"leftAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"leftAbility\"] = 0\n",
        "    abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "    # Goal Ahead Ability\n",
        "    aheadAbility = scaledBeta(\"aheadAbility\", 1,1, 0, 1, simpleGraph=False)\n",
        "    abilityMin[\"aheadAbility\"] = 0\n",
        "    abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "    ## Environment variables as Deterministic (about the instance)\n",
        "\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", taskResults[\"lavaPresence\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", taskResults[\"numChoices\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\",taskResults[\"mainGoalSize\"].values)\n",
        "    goalDist = pm.MutableData(\"goalDistance\", taskResults[\"minDistToGoal\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", taskResults[\"minNumTurnsGoal\"])\n",
        "    choiceDist = pm.MutableData(\"choiceDistance\", taskResults[\"minDistToCorrectChoice\"].values)\n",
        "    numTurnsChoice = pm.MutableData(\"minTurnsToCorrectChoice\", taskResults[\"minNumTurnsChoice\"].values)\n",
        "    goalRight = pm.MutableData(\"goalRight\", taskResults[\"goalRightRelToStart\"])\n",
        "    goalAhead = pm.MutableData(\"goalAhead\", taskResults[\"goalCentreRelToStart\"])\n",
        "    goalLeft = pm.MutableData(\"goalLeft\", taskResults[\"goalLeftRelToStart\"])\n",
        "    opTest = pm.MutableData(\"allocentricOPTest\", taskResults[\"goalBecomesAllocentricallyOccluded\"].values)\n",
        "\n",
        "    ## Margins\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic(visualAcuityLOMargin(visualAcuityAbility, goalSize, goalDist)))\n",
        "\n",
        "    rightP = pm.Deterministic(\"rightPerformance\", logistic999(SimplePrMargin(rightAbility, goalRight), min = 0, max = 1))\n",
        "    aheadP = pm.Deterministic(\"aheadPerformance\", logistic999(SimplePrMargin(aheadAbility, goalAhead), min = 0, max = 1))\n",
        "    leftP = pm.Deterministic(\"leftPerformance\", logistic999(SimplePrMargin(leftAbility, goalLeft), min = 0, max = 1))\n",
        "\n",
        "    flatNavSuccessP = pm.Deterministic(\"flatNavSuccessP\", logistic999(flatNavAbility - (numTurnsGoal * goalDist), min = minFlatNav, max = maxFlatNav))\n",
        "    flatNavChoiceP = pm.Deterministic(\"flatNavChoiceP\", logistic999(flatNavAbility - (numTurnsChoice * choiceDist), min = minFlatNav, max = maxFlatNav))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(SimplePrMargin(lavaAbility, lavaPresence), min = 0, max = 1))\n",
        "\n",
        "    navSuccessP = pm.Deterministic(\"navSuccessP\", (lavaP * flatNavSuccessP))\n",
        "    navChoiceP = pm.Deterministic(\"navChoiceP\", (flatNavChoiceP * aheadP * leftP * aheadP))\n",
        "\n",
        "    OPPerformance = (objPermAbility  - (((goalDist*opTest) * numChoices) + (numChoices * (1-opTest))))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", (logistic999(OPPerformance, min = minPermAbility, max = maxPermAbility)))\n",
        "\n",
        "    choiceP = pm.Deterministic(\"choiceP\", (navChoiceP * objPermP))\n",
        "    successP = pm.Deterministic(\"successP\", (visualAcuityP * navSuccessP * objPermP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", successP, observed=results)\n",
        "    taskChoice = pm.Bernoulli(\"taskChoice\", choiceP, observed=choices)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "isUjy5PMB8_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = SetupMultivariateBasicModel(agent_data, uniformAbilitySlack=1, agent_type='agent_real', agent_name_success =\"success_ppo_bc_all_2023\", agent_name_choice = \"correctChoice_ppo_bc_all_2023\", sample = None)\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv.format = \"png\"\n",
        "gv.render(filename='viz/MultivariateNonHierarchicalMeasurementLayout.gv', directory='/content/drive/Shareddrives/comparative-object-permanence')\n",
        "Image(\"/content/drive/Shareddrives/comparative-object-permanence/MultivariateNonHierarchicalMeasurementLayout.gv.png\")\n",
        "gv"
      ],
      "metadata": {
        "id": "tTxAqkOyG6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SetupMultivariateHierarchicalModel(taskResults, uniformAbilitySlack = 1):\n",
        "  \"\"\"\n",
        "  taskResults is the conjunction of the metadata with the successes of the agents on that set of tests. Needs to be in long format for hierarchical model\n",
        "  \"\"\"\n",
        "  assert uniformAbilitySlack >=1, \"Slack must be greater than or equal to 1.\"\n",
        "\n",
        "\n",
        "  agent_idx, agents = taskResults['agent_tag'].factorize(sort=True)\n",
        "\n",
        "  COORDS = {\n",
        "    'obs': taskResults.index,\n",
        "    'agent': agents ##only by type of agent, not by random seed.\n",
        "    }\n",
        "\n",
        "  abilityMin = {} #Initialize ability Min and max dataframes for plotting ranges\n",
        "  abilityMax = {}\n",
        "\n",
        "  #Decide \"maximum capabilities\" based on the hardest values in the dataset\n",
        "  maxDistanceGoal = taskResults[\"minDistToGoal\"].max()\n",
        "  maxTurnsGoal = taskResults[\"minNumTurnsGoal\"].max()\n",
        "  maxChoices = taskResults[\"numChoices\"].max()\n",
        "  maxGoalSize = taskResults[\"mainGoalSize\"].max() * uniformAbilitySlack\n",
        "  maxPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).max()) * uniformAbilitySlack\n",
        "  maxDistanceChoice = taskResults[\"minDistToCorrectChoice\"].max()\n",
        "  maxTurnsChoice = taskResults[\"minNumTurnsChoice\"].max()\n",
        "\n",
        "  maxChoiceNav = ((taskResults[\"minDistToCorrectChoice\"]*taskResults[\"minNumTurnsChoice\"])).max() * uniformAbilitySlack # instrumental, not used in measurement layout explicitly\n",
        "  maxSuccessNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).max() * uniformAbilitySlack # instrumental, not used in measurement layout explicitly\n",
        "\n",
        "  maxFlatNav = max([maxChoiceNav, maxSuccessNav]) # pick the largest of the two products\n",
        "\n",
        "\n",
        "  # Decide 'minimum capabilities' based on easiest values in the dataset\n",
        "  minDistanceGoal = taskResults[\"minDistToGoal\"].min()\n",
        "  minTurnsGoal = taskResults[\"minNumTurnsGoal\"].min()\n",
        "  minChoices = taskResults[\"numChoices\"].min()\n",
        "  minGoalSize = taskResults[\"mainGoalSize\"].min() * (1-(uniformAbilitySlack-1))\n",
        "  minPermAbility = ((taskResults[\"minDistToGoal\"] * taskResults[\"numChoices\"]).min()) * (1-(uniformAbilitySlack-1))\n",
        "  minDistanceChoice = taskResults[\"minDistToCorrectChoice\"].min()\n",
        "  minTurnsChoice = taskResults[\"minNumTurnsChoice\"].min()\n",
        "\n",
        "  minChoiceNav = ((taskResults[\"minDistToCorrectChoice\"]*taskResults[\"minNumTurnsChoice\"])).min() * (1-(uniformAbilitySlack-1)) # instrumental, not used in measurement layout explicitly\n",
        "  minSuccessNav = ((taskResults[\"minDistToGoal\"]*taskResults[\"minNumTurnsGoal\"])).min() * (1-(uniformAbilitySlack-1)) # instrumental, not used in measurement layout explicitly\n",
        "\n",
        "  minFlatNav = min([minChoiceNav, minSuccessNav])\n",
        "\n",
        "  m = pm.Model(coords=COORDS)\n",
        "  with m:\n",
        "\n",
        "    #### Hyper priors ####\n",
        "    V_Lava = pm.Beta('V_Lava', 1, 1)\n",
        "    U_Lava = pm.HalfNormal('U_Lava',sigma = 10)\n",
        "    V_Ahead = pm.Beta('V_Ahead', 1, 1)\n",
        "    U_Ahead = pm.HalfNormal('U_Ahead',sigma = 10)\n",
        "    V_Left = pm.Beta('V_Left', 1, 1)\n",
        "    U_Left = pm.HalfNormal('U_Left',sigma = 10)\n",
        "    V_Right = pm.Beta('V_Right', 1, 1)\n",
        "    U_Right = pm.HalfNormal('U_Right',sigma = 10)\n",
        "\n",
        "    V_OP = pm.Beta('V_OP', 1, 1)\n",
        "    U_OP = pm.HalfNormal('U_OP',sigma = 10)\n",
        "    V_VisualAcuity = pm.Beta('V_VisualAcuity', 1, 1)\n",
        "    U_VisualAcuity = pm.HalfNormal('U_VisualAcuity',sigma = 10)\n",
        "    V_FlatNav = pm.Beta('V_FlatNav', 1, 1)\n",
        "    U_FlatNav = pm.HalfNormal('U_FlatNav',sigma = 10)\n",
        "\n",
        "    ##ignore noise for now.\n",
        "\n",
        "    ### Ability Priors as scaled beta distributions\n",
        "    al_Lava = pm.Deterministic('al_Lava', V_Lava * U_Lava)\n",
        "    be_Lava = pm.Deterministic('be_Lava', (1-V_Lava) * U_Lava)\n",
        "    al_Ahead = pm.Deterministic('al_Ahead', V_Ahead * U_Ahead)\n",
        "    be_Ahead = pm.Deterministic('be_Ahead', (1-V_Ahead) * U_Ahead)\n",
        "    al_Left = pm.Deterministic('al_Left', V_Left * U_Left)\n",
        "    be_Left = pm.Deterministic('be_Left', (1-V_Left) * U_Left)\n",
        "    al_Right = pm.Deterministic('al_Right', V_Right * U_Right)\n",
        "    be_Right = pm.Deterministic('be_Right', (1-V_Right) * U_Right)\n",
        "\n",
        "    al_OP = pm.Deterministic('al_OP', V_OP * U_OP)\n",
        "    be_OP = pm.Deterministic('be_OP', (1-V_OP) * U_OP)\n",
        "    al_VisualAcuity = pm.Deterministic('al_VisualAcuity', V_VisualAcuity * U_VisualAcuity)\n",
        "    be_VisualAcuity = pm.Deterministic('be_VisualAcuity', (1-V_VisualAcuity) * U_VisualAcuity)\n",
        "    al_FlatNav = pm.Deterministic('al_FlatNav', V_FlatNav * U_FlatNav)\n",
        "    be_FlatNav = pm.Deterministic('be_FlatNav', (1-V_FlatNav) * U_FlatNav)\n",
        "\n",
        "    # Parametrise the beta distributions\n",
        "    z1_Lava = pm.Gamma(\"z1_Lava\", alpha=al_Lava, beta=1, dims = 'agent')\n",
        "    z2_Lava = pm.Gamma(\"z2_Lava\", alpha=be_Lava, beta=1, dims = 'agent')\n",
        "    z1_Ahead = pm.Gamma(\"z1_Ahead\", alpha=al_Ahead, beta=1, dims = 'agent')\n",
        "    z2_Ahead = pm.Gamma(\"z2_Ahead\", alpha=be_Ahead, beta=1, dims = 'agent')\n",
        "    z1_Left = pm.Gamma(\"z1_Left\", alpha=al_Left, beta=1, dims = 'agent')\n",
        "    z2_Left = pm.Gamma(\"z2_Left\", alpha=be_Left, beta=1, dims = 'agent')\n",
        "    z1_Right = pm.Gamma(\"z1_Right\", alpha=al_Right, beta=1, dims = 'agent')\n",
        "    z2_Right = pm.Gamma(\"z2_Right\", alpha=be_Right, beta=1, dims = 'agent')\n",
        "\n",
        "    z1_OP = pm.Gamma(\"z1_OP\", alpha=al_OP, beta=1, dims = 'agent')\n",
        "    z2_OP = pm.Gamma(\"z2_OP\", alpha=be_OP, beta=1, dims = 'agent')\n",
        "    z1_VisualAcuity = pm.Gamma(\"z1_VisualAcuity\", alpha=al_VisualAcuity, beta=1, dims = 'agent')\n",
        "    z2_VisualAcuity = pm.Gamma(\"z2_VisualAcuity\", alpha=be_VisualAcuity, beta=1, dims = 'agent')\n",
        "    z1_FlatNav = pm.Gamma(\"z1_FlatNav\", alpha=al_FlatNav, beta=1, dims = 'agent')\n",
        "    z2_FlatNav = pm.Gamma(\"z2_FlatNav\", alpha=be_FlatNav, beta=1, dims = 'agent')\n",
        "\n",
        "    ## Abilities\n",
        "\n",
        "    lavaAbility = pm.Deterministic('lavaAbility',(z1_Lava/(z1_Lava+z2_Lava)),dims='agent')\n",
        "    abilityMin[\"lavaAbility\"] = 0\n",
        "    abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "    aheadAbility = pm.Deterministic('aheadAbility',(z1_Ahead/(z1_Ahead+z2_Ahead)),dims='agent')\n",
        "    abilityMin[\"aheadAbility\"] = 0\n",
        "    abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "    leftAbility = pm.Deterministic('leftAbility',(z1_Left/(z1_Left+z2_Left)),dims='agent')\n",
        "    abilityMin[\"leftAbility\"] = 0\n",
        "    abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "    rightAbility = pm.Deterministic('rightAbility',(z1_Right/(z1_Right+z2_Right)),dims='agent')\n",
        "    abilityMin[\"rightAbility\"] = 0\n",
        "    abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "    objPermAbility = pm.Deterministic(\"objPermAbility\", ((z1_OP/(z1_OP+z2_OP))*maxPermAbility)+minPermAbility, dims='agent')  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "    abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "    abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "    abilityMin[\"visualAcuityAbility\"] = minDistanceGoal/maxGoalSize\n",
        "    abilityMax[\"visualAcuityAbility\"] = minDistanceGoal/minGoalSize\n",
        "    visualAcuityAbility = pm.Deterministic(\"visualAcuityAbility\", ((z1_VisualAcuity/(z1_VisualAcuity+z2_VisualAcuity))*abilityMax[\"visualAcuityAbility\"])+abilityMin[\"visualAcuityAbility\"], dims='agent')  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "\n",
        "    abilityMin[\"flatNavAbility\"] = minFlatNav\n",
        "    abilityMax[\"flatNavAbility\"] = maxFlatNav\n",
        "    flatNavAbility = pm.Deterministic(\"flatNavAbility\", ((z1_FlatNav/(z1_FlatNav+z2_FlatNav))*maxFlatNav)+minFlatNav, dims='agent')  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "\n",
        "    ## Environment variables as Deterministic (about the instance)\n",
        "\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", taskResults[\"lavaPresence\"].values, dims='obs')\n",
        "    numChoices = pm.MutableData(\"numChoices\", taskResults[\"numChoices\"].values, dims='obs')\n",
        "    goalSize = pm.MutableData(\"goalSize\",taskResults[\"mainGoalSize\"].values, dims='obs')\n",
        "    goalDist = pm.MutableData(\"goalDistance\", taskResults[\"minDistToGoal\"].values, dims='obs')\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", taskResults[\"minNumTurnsGoal\"], dims='obs')\n",
        "    choiceDist = pm.MutableData(\"choiceDistance\", taskResults[\"minDistToCorrectChoice\"].values, dims='obs')\n",
        "    numTurnsChoice = pm.MutableData(\"minTurnsToCorrectChoice\", taskResults[\"minNumTurnsChoice\"].values, dims='obs')\n",
        "    goalRight = pm.MutableData(\"goalRight\", taskResults[\"goalRightRelToStart\"], dims='obs')\n",
        "    goalAhead = pm.MutableData(\"goalAhead\", taskResults[\"goalCentreRelToStart\"], dims='obs')\n",
        "    goalLeft = pm.MutableData(\"goalLeft\", taskResults[\"goalLeftRelToStart\"], dims='obs')\n",
        "    opTest = pm.MutableData(\"allocentricOPTest\", taskResults[\"goalBecomesAllocentricallyOccluded\"].values, dims='obs')\n",
        "\n",
        "    ## Margins\n",
        "\n",
        "    rightP = pm.Deterministic(\"rightPerformance\", logistic999(SimplePrMargin(rightAbility[agent_idx], goalRight), min = 0, max = 1),dims='obs')\n",
        "    aheadP = pm.Deterministic(\"aheadPerformance\", logistic999(SimplePrMargin(aheadAbility[agent_idx], goalAhead), min = 0, max = 1),dims='obs')\n",
        "    leftP = pm.Deterministic(\"leftPerformance\", logistic999(SimplePrMargin(leftAbility[agent_idx], goalLeft), min = 0, max = 1),dims='obs')\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic(visualAcuityLOMargin(visualAcuityAbility[agent_idx], goalSize, goalDist)),dims='obs')\n",
        "\n",
        "    flatNavSuccessP = pm.Deterministic(\"flatNavSuccessP\", logistic999(flatNavAbility[agent_idx] - (numTurnsGoal * goalDist), min = minFlatNav, max = maxFlatNav),dims='obs')\n",
        "    flatNavChoiceP = pm.Deterministic(\"flatNavChoiceP\", logistic999(flatNavAbility[agent_idx] - (numTurnsChoice * choiceDist), min = minFlatNav, max = maxFlatNav),dims='obs')\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(SimplePrMargin(lavaAbility[agent_idx], lavaPresence), min = 0, max = 1),dims='obs')\n",
        "\n",
        "    navSuccessP = pm.Deterministic(\"navSuccessP\", (lavaP * flatNavSuccessP),dims='obs')\n",
        "    navChoiceP = pm.Deterministic(\"navChoiceP\", (flatNavChoiceP * aheadP * leftP * rightP),dims='obs')\n",
        "\n",
        "    OPPerformance = pm.Deterministic(\"OPPerf\", objPermAbility[agent_idx]  - (((goalDist*opTest) * numChoices) + (numChoices * (1-opTest))), dims='obs') #if there are multiple choices, the agent has to have some object permanence to solve the task.\n",
        "    objPermP = pm.Deterministic(\"objPermP\", (logistic999(OPPerformance, min = minPermAbility, max = maxPermAbility)),dims='obs')\n",
        "\n",
        "    choiceP = pm.Deterministic(\"choiceP\", (navChoiceP * objPermP),dims='obs')\n",
        "    successP = pm.Deterministic(\"successP\", (visualAcuityP * navSuccessP * objPermP),dims='obs')\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", successP, observed=taskResults['success'].values,dims='obs')\n",
        "    taskChoice = pm.Bernoulli(\"taskChoice\", choiceP, observed=taskResults['correctChoice'].values,dims='obs')\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "Dugxf3QnPrAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_data = long_data[(long_data['agent_type_gen'].str.contains(\"Child\"))].dropna(subset = ['success', 'correctChoice'])"
      ],
      "metadata": {
        "id": "8wVUqeB6VFxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = SetupMultivariateHierarchicalModel(agent_data, uniformAbilitySlack=1)\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv.format = \"png\"\n",
        "gv.render(filename='viz/MultivariateHierarchicalMeasurementLayout.gv', directory='/content/drive/Shareddrives/comparative-object-permanence')\n",
        "Image(\"/content/drive/Shareddrives/comparative-object-permanence/MultivariateHierarchicalMeasurementLayout.gv.png\")\n",
        "gv"
      ],
      "metadata": {
        "id": "vSVcvmb1RnX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Complete Multivariate Measurement Layouts"
      ],
      "metadata": {
        "id": "ivfCdNnU4gel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_dreamer_names = [\"dreamer_bc_all_9999\",\n",
        "                     \"dreamer_bc_opc_all_9999\",\n",
        "                     \"dreamer_bc_opc_strat_9999\",\n",
        "                     \"dreamer_bc_opc_opt_all_9999\",\n",
        "                     \"dreamer_bc_opc_opt_strat_9999\",\n",
        "                     \"ppo_bc_all_2023\",\n",
        "                     \"ppo_bc_opc_strat_2023\",\n",
        "                     \"ppo_bc_opc_all_2023\",\n",
        "                     \"ppo_bc_opc_opt_strat_2023\",\n",
        "                     \"ppo_bc_opc_opt_all_2023\"]\n",
        "\n",
        "\n",
        "synthetic_agent_names = [\"perfectAgent\",\n",
        "                         \"failedAgent\",\n",
        "                         \"noOPAgent\",\n",
        "                         \"lowVisualAcuityAgent\",\n",
        "                         \"poorNavigationOPAgent\",\n",
        "                         \"poorLavaOPAgent\"]\n",
        "\n",
        "\n",
        "reference_agents_types = [\"Child\",\n",
        "                          \"Heuristic Agent\",\n",
        "                          \"Random Agent\"]\n"
      ],
      "metadata": {
        "id": "PcNAxy4CqUZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_agents_training, synthetic_agents_test = train_test_split(synthetic_agents_data, test_size = 0.2)\n",
        "RL_agents_training, RL_agents_test = train_test_split(agent_data, test_size=0.2)"
      ],
      "metadata": {
        "id": "Xlfsz3f63lJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abilitiesToShow = [\"objPermAbility\", \"visualAcuityAbility\", \"flatNavAbility\", \"lavaAbility\", \"rightAbility\", \"leftAbility\", \"aheadAbility\"]\n",
        "\n",
        "pymc_sample_num = 2000\n",
        "\n",
        "slack = 1"
      ],
      "metadata": {
        "id": "LZ2sA86g3lGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentName = []\n",
        "modelBrierScoreSuccess = []\n",
        "modelBrierScoreChoice = []\n",
        "aggregateBrierScoreSuccess = []\n",
        "aggregateBrierScoreChoice = []\n",
        "modelCalibrationSuccess = []\n",
        "modelCalibrationChoice = []\n",
        "aggregateCalibrationSuccess = []\n",
        "aggregateCalibrationChoice = []\n",
        "modelRefinementSuccess = []\n",
        "modelRefinementChoice = []\n",
        "aggregateRefinementSuccess = []\n",
        "aggregateRefinementChoice = []\n",
        "modelBetterThanAggSuccess = []\n",
        "modelBetterThanAggChoice = []\n",
        "meanSuccess = []\n",
        "meanChoice = []\n",
        "\n",
        "OPAbilityAllMean = []\n",
        "navAbilityAllMean = []\n",
        "visualAcuityAbilityAllMean = []\n",
        "flatNavAbilityAllMean = []\n",
        "lavaAbilityAllMean = []\n",
        "rightAbilityAllMean = []\n",
        "leftAbilityAllMean = []\n",
        "aheadAbilityAllMean = []\n",
        "\n",
        "OPAbilityAllSD = []\n",
        "navAbilityAllSD = []\n",
        "visualAcuityAbilityAllSD = []\n",
        "flatNavAbilityAllSD = []\n",
        "lavaAbilityAllSD = []\n",
        "rightAbilityAllSD = []\n",
        "leftAbilityAllSD = []\n",
        "aheadAbilityAllSD = []\n",
        "\n",
        "OPAbilityAllMin = []\n",
        "navAbilityAllMin = []\n",
        "visualAcuityAbilityAllMin = []\n",
        "flatNavAbilityAllMin = []\n",
        "lavaAbilityAllMin = []\n",
        "rightAbilityAllMin = []\n",
        "leftAbilityAllMin = []\n",
        "aheadAbilityAllMin = []\n",
        "\n",
        "OPAbilityAllMax = []\n",
        "navAbilityAllMax = []\n",
        "visualAcuityAbilityAllMax = []\n",
        "flatNavAbilityAllMax = []\n",
        "lavaAbilityAllMax = []\n",
        "rightAbilityAllMax = []\n",
        "leftAbilityAllMax = []\n",
        "aheadAbilityAllMax = []"
      ],
      "metadata": {
        "id": "TVZLICbM-PPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_model_folder = '/content/drive/Shareddrives/comparative-object-permanence/measurement_layout_results/training_set_model'\n",
        "full_data_model_folder = '/content/drive/Shareddrives/comparative-object-permanence/measurement_layout_results/full_data_model'\n",
        "figures_folder = '/content/drive/Shareddrives/comparative-object-permanence/measurement_layout_results/figures'"
      ],
      "metadata": {
        "id": "mVWu-IDmv5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for agent in synthetic_agent_names:\n",
        "  model_train, ability_min, ability_max = SetupMultivariateBasicModel(synthetic_agents_training, uniformAbilitySlack=1, agent_type='agent_synthetic', agent_name_success = f\"{agent}_success\", agent_name_choice = f\"{agent}_choice\", sample = None)\n",
        "  with model_train:\n",
        "    data_training = pm.sample(pymc_sample_num, target_accept=0.95)\n",
        "\n",
        "  #save_object(model_train, f\"{training_data_model_folder}/model_{agent}.pkl\") #save the model just in case we need it again.\n",
        "\n",
        "  predictionsSuccessInstance, predictionsChoiceInstance, successes, choices = predict(model_train, data_training, synthetic_agents_test, f\"{agent}_success\", f\"{agent}_choice\", len(synthetic_agents_training), multi_var=True)\n",
        "\n",
        "  agentBrierScoreSuccess, agentCalibrationSuccess, agentRefinementSuccess = brierDecomp(predictionsSuccessInstance, successes)\n",
        "  agentAggBrierScoreSuccess, agentAggCalibrationSuccess, agentAggRefinementSuccess = brierDecomp(np.repeat(np.mean(synthetic_agents_training[f\"{agent}_success\"]), len(successes)), successes)\n",
        "  agentBrierScoreChoice, agentCalibrationChoice, agentRefinementChoice = brierDecomp(predictionsChoiceInstance, choices)\n",
        "  agentAggBrierScoreChoice, agentAggCalibrationChoice, agentAggRefinementChoice = brierDecomp(np.repeat(np.mean(synthetic_agents_training[f\"{agent}_choice\"]), len(choices)), choices)\n",
        "\n",
        "  aggregateBrierScoreSuccess.append(agentAggBrierScoreSuccess)\n",
        "  aggregateBrierScoreChoice.append(agentAggBrierScoreChoice)\n",
        "  aggregateCalibrationSuccess.append(agentAggCalibrationSuccess)\n",
        "  aggregateCalibrationChoice.append(agentAggCalibrationChoice)\n",
        "  aggregateRefinementSuccess.append(agentAggRefinementSuccess)\n",
        "  aggregateRefinementChoice.append(agentAggRefinementChoice)\n",
        "\n",
        "  modelBrierScoreSuccess.append(agentBrierScoreSuccess)\n",
        "  modelBrierScoreChoice.append(agentBrierScoreChoice)\n",
        "  modelCalibrationSuccess.append(agentCalibrationSuccess)\n",
        "  modelCalibrationChoice.append(agentCalibrationChoice)\n",
        "  modelRefinementSuccess.append(agentRefinementSuccess)\n",
        "  modelRefinementChoice.append(agentRefinementChoice)\n",
        "\n",
        "  if agentBrierScoreSuccess < agentAggBrierScoreSuccess:\n",
        "    modelBetterThanAggSuccess.append(True)\n",
        "  else:\n",
        "    modelBetterThanAggSuccess.append(False)\n",
        "\n",
        "  if agentBrierScoreChoice < agentAggBrierScoreChoice:\n",
        "    modelBetterThanAggChoice.append(True)\n",
        "  else:\n",
        "    modelBetterThanAggChoice.append(False)\n",
        "\n",
        "  # now train on all data\n",
        "\n",
        "  model_all, ability_min, ability_max = SetupMultivariateBasicModel(synthetic_agents_data, uniformAbilitySlack=1, agent_type='agent_synthetic', agent_name_success = f\"{agent}_success\", agent_name_choice = f\"{agent}_choice\", sample = None)\n",
        "  with model_all:\n",
        "    data_all = pm.sample(pymc_sample_num, target_accept=0.95)\n",
        "\n",
        "  #save_object(data_all, f\"{full_data_model_folder}/model_{agent}.pkl\") #save the model just in case we need it again.\n",
        "\n",
        "  mu, sd  = analyzeAgentResults(data_all, abilitiesToShow)\n",
        "\n",
        "  OPAbilityAllMean.append(mu[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMean.append(mu[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMean.append(mu[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMean.append(mu[\"lavaAbility\"])\n",
        "  rightAbilityAllMean.append(mu[\"rightAbility\"])\n",
        "  leftAbilityAllMean.append(mu[\"leftAbility\"])\n",
        "  aheadAbilityAllMean.append(mu[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllSD.append(sd[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllSD.append(sd[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllSD.append(sd[\"flatNavAbility\"])\n",
        "  lavaAbilityAllSD.append(sd[\"lavaAbility\"])\n",
        "  rightAbilityAllSD.append(sd[\"rightAbility\"])\n",
        "  leftAbilityAllSD.append(sd[\"leftAbility\"])\n",
        "  aheadAbilityAllSD.append(sd[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllMin.append(ability_min[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMin.append(ability_min[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMin.append(ability_min[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMin.append(ability_min[\"lavaAbility\"])\n",
        "  rightAbilityAllMin.append(ability_min[\"rightAbility\"])\n",
        "  leftAbilityAllMin.append(ability_min[\"leftAbility\"])\n",
        "  aheadAbilityAllMin.append(ability_min[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllMax.append(ability_max[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMax.append(ability_max[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMax.append(ability_max[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMax.append(ability_max[\"lavaAbility\"])\n",
        "  rightAbilityAllMax.append(ability_max[\"rightAbility\"])\n",
        "  leftAbilityAllMax.append(ability_max[\"leftAbility\"])\n",
        "  aheadAbilityAllMax.append(ability_max[\"aheadAbility\"])\n",
        "\n",
        "  meanSuccess.append(np.mean(synthetic_agents_data[f\"{agent}_success\"]))\n",
        "  meanChoice.append(np.mean(synthetic_agents_data[f\"{agent}_choice\"]))\n",
        "\n",
        "  agentName.append(agent)\n",
        "\n",
        "  finalDF = pd.DataFrame({\"Agent Name\": agentName,\n",
        "                          \"Model Brier Score Success\":modelBrierScoreSuccess,\n",
        "                          \"Model Brier Score Choice\":modelBrierScoreChoice,\n",
        "                          \"Aggregate Brier Score Success\": aggregateBrierScoreSuccess,\n",
        "                          \"Aggregate Brier Score Choice\": aggregateBrierScoreChoice,\n",
        "                          \"Model Calibration Success\": modelCalibrationSuccess,\n",
        "                          \"Model Calibration Choice\": modelCalibrationChoice,\n",
        "                          \"Aggregate Calibration Success\":aggregateCalibrationSuccess,\n",
        "                          \"Aggregate Calibration Choice\":aggregateCalibrationChoice,\n",
        "                          \"Model Refinement Success\" : modelRefinementSuccess,\n",
        "                          \"Model Refinement Choice\" : modelRefinementChoice,\n",
        "                          \"Aggregate Refinement Success\" : aggregateRefinementSuccess,\n",
        "                          \"Aggregate Refinement Choice\" : aggregateRefinementChoice,\n",
        "                          \"Model Better? (Based on Brier Score - Success)\":modelBetterThanAggSuccess,\n",
        "                          \"Model Better? (Based on Brier Score - Choice)\":modelBetterThanAggChoice,\n",
        "                          \"Average Success\":meanSuccess,\n",
        "                          \"Average Correct Chocie\":meanChoice,\n",
        "                          \"Object Permanence Ability Mean (All)\": OPAbilityAllMean,\n",
        "                          \"Visual Acuity Ability Mean (All)\" : visualAcuityAbilityAllMean,\n",
        "                          \"Flat Navigation Ability Mean (All)\" : flatNavAbilityAllMean,\n",
        "                          \"Lava Ability Mean (All)\" : lavaAbilityAllMean,\n",
        "                          \"Right Ability Mean (All)\" : rightAbilityAllMean,\n",
        "                          \"Left Ability Mean (All)\" : leftAbilityAllMean,\n",
        "                          \"Ahead Ability Mean (All)\" : aheadAbilityAllMean,\n",
        "                          \"Object Permanence Ability SD (All)\": OPAbilityAllSD,\n",
        "                          \"Visual Acuity Ability SD (All)\" : visualAcuityAbilityAllSD,\n",
        "                          \"Flat Navigation Ability SD (All)\" : flatNavAbilityAllSD,\n",
        "                          \"Lava Ability SD (All)\" : lavaAbilityAllSD,\n",
        "                          \"Right Ability SD (All)\" : rightAbilityAllSD,\n",
        "                          \"Left Ability SD (All)\" : leftAbilityAllSD,\n",
        "                          \"Ahead Ability SD (All)\" : aheadAbilityAllSD,\n",
        "                          \"Object Permanence Ability Min (All)\": OPAbilityAllMin,\n",
        "                          \"Visual Acuity Ability Min (All)\" : visualAcuityAbilityAllMin,\n",
        "                          \"Flat Navigation Ability Min (All)\" : flatNavAbilityAllMin,\n",
        "                          \"Lava Ability Min (All)\" : lavaAbilityAllMin,\n",
        "                          \"Right Ability Min (All)\" : rightAbilityAllMin,\n",
        "                          \"Left Ability Min (All)\" : leftAbilityAllMin,\n",
        "                          \"Ahead Ability Min (All)\" : aheadAbilityAllMin,\n",
        "                          \"Object Permanence Ability Max (All)\": OPAbilityAllMax,\n",
        "                          \"Visual Acuity Ability Max (All)\" : visualAcuityAbilityAllMax,\n",
        "                          \"Flat Navigation Ability Max (All)\" : flatNavAbilityAllMax,\n",
        "                          \"Lava Ability Max (All)\" : lavaAbilityAllMax,\n",
        "                          \"Right Ability Max (All)\" : rightAbilityAllMax,\n",
        "                          \"Left Ability Max (All)\" : leftAbilityAllMax,\n",
        "                          \"Ahead Ability Max (All)\" : aheadAbilityAllMax,})\n",
        "  finalDF.to_csv(\"/content/drive/Shareddrives/comparative-object-permanence/measurement_layout_results/AgentResults.csv\", index = False)\n",
        "\n",
        "  ## figures\n",
        "\n",
        "  for ability in abilitiesToShow:\n",
        "    trace_plot = az.plot_trace(data=data_all['posterior'][[ability]])\n",
        "    plt.savefig(f\"{figures_folder}/traceplot_{agent}_{ability}.png\")\n",
        "\n",
        "    forest_plot = az.plot_forest(data=data_all['posterior'][[ability]])\n",
        "    plt.savefig(f\"{figures_folder}/forest_{agent}_{ability}.png\")\n",
        "\n",
        "  energy_plot = az.plot_energy(data=data_all)\n",
        "  plt.savefig(f\"{figures_folder}/energyplot_{agent}.png\")\n",
        "\n",
        "  summary = az.summary(data_all['posterior'][abilitiesToShow])\n",
        "  save_object(summary, f\"{full_data_model_folder}/summary_{agent}.pkl\")\n",
        "  summary.to_csv(f\"{full_data_model_folder}/summary_{agent}.csv\", index = False)\n",
        "\n",
        "  print(f\"Agent: {agent}\")\n",
        "  print(summary)\n"
      ],
      "metadata": {
        "id": "RcYu10Z73lD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for agent in ppo_dreamer_names:\n",
        "  model_train, ability_min, ability_max = SetupMultivariateBasicModel(RL_agents_training, uniformAbilitySlack=1, agent_type='agent_real', agent_name_success = f\"success_{agent}\", agent_name_choice = f\"correctChoice_{agent}\", sample = None)\n",
        "  with model_train:\n",
        "    data_training = pm.sample(pymc_sample_num, target_accept=0.95)\n",
        "\n",
        "  #save_object(model_train, f\"{training_data_model_folder}/model_{agent}.pkl\") #save the model just in case we need it again.\n",
        "\n",
        "  predictionsSuccessInstance, predictionsChoiceInstance, successes, choices = predict(model_train, data_training, RL_agents_test, f\"success_{agent}\", f\"correctChoice_{agent}\", len(RL_agents_training), multi_var=True)\n",
        "\n",
        "  agentBrierScoreSuccess, agentCalibrationSuccess, agentRefinementSuccess = brierDecomp(predictionsSuccessInstance, successes)\n",
        "  agentAggBrierScoreSuccess, agentAggCalibrationSuccess, agentAggRefinementSuccess = brierDecomp(np.repeat(np.mean(RL_agents_training[f\"success_{agent}\"]), len(successes)), successes)\n",
        "  agentBrierScoreChoice, agentCalibrationChoice, agentRefinementChoice = brierDecomp(predictionsChoiceInstance, choices)\n",
        "  agentAggBrierScoreChoice, agentAggCalibrationChoice, agentAggRefinementChoice = brierDecomp(np.repeat(np.mean(RL_agents_training[f\"correctChoice_{agent}\"]), len(choices)), choices)\n",
        "\n",
        "  aggregateBrierScoreSuccess.append(agentAggBrierScoreSuccess)\n",
        "  aggregateBrierScoreChoice.append(agentAggBrierScoreChoice)\n",
        "  aggregateCalibrationSuccess.append(agentAggCalibrationSuccess)\n",
        "  aggregateCalibrationChoice.append(agentAggCalibrationChoice)\n",
        "  aggregateRefinementSuccess.append(agentAggRefinementSuccess)\n",
        "  aggregateRefinementChoice.append(agentAggRefinementChoice)\n",
        "\n",
        "  modelBrierScoreSuccess.append(agentBrierScoreSuccess)\n",
        "  modelBrierScoreChoice.append(agentBrierScoreChoice)\n",
        "  modelCalibrationSuccess.append(agentCalibrationSuccess)\n",
        "  modelCalibrationChoice.append(agentCalibrationChoice)\n",
        "  modelRefinementSuccess.append(agentRefinementSuccess)\n",
        "  modelRefinementChoice.append(agentRefinementChoice)\n",
        "\n",
        "  if agentBrierScoreSuccess < agentAggBrierScoreSuccess:\n",
        "    modelBetterThanAggSuccess.append(True)\n",
        "  else:\n",
        "    modelBetterThanAggSuccess.append(False)\n",
        "\n",
        "  if agentBrierScoreChoice < agentAggBrierScoreChoice:\n",
        "    modelBetterThanAggChoice.append(True)\n",
        "  else:\n",
        "    modelBetterThanAggChoice.append(False)\n",
        "\n",
        "  # now train on all data\n",
        "\n",
        "  model_all, ability_min, ability_max = SetupMultivariateBasicModel(agent_data, uniformAbilitySlack=1, agent_type='agent_real', agent_name_success = f\"success_{agent}\", agent_name_choice = f\"correctChoice_{agent}\", sample = None)\n",
        "  with model_all:\n",
        "    data_all = pm.sample(pymc_sample_num, target_accept=0.95)\n",
        "\n",
        "  #save_object(data_all, f\"{full_data_model_folder}/model_{agent}.pkl\") #save the model just in case we need it again.\n",
        "\n",
        "  mu, sd  = analyzeAgentResults(data_all, abilitiesToShow)\n",
        "\n",
        "  OPAbilityAllMean.append(mu[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMean.append(mu[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMean.append(mu[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMean.append(mu[\"lavaAbility\"])\n",
        "  rightAbilityAllMean.append(mu[\"rightAbility\"])\n",
        "  leftAbilityAllMean.append(mu[\"leftAbility\"])\n",
        "  aheadAbilityAllMean.append(mu[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllSD.append(sd[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllSD.append(sd[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllSD.append(sd[\"flatNavAbility\"])\n",
        "  lavaAbilityAllSD.append(sd[\"lavaAbility\"])\n",
        "  rightAbilityAllSD.append(sd[\"rightAbility\"])\n",
        "  leftAbilityAllSD.append(sd[\"leftAbility\"])\n",
        "  aheadAbilityAllSD.append(sd[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllMin.append(ability_min[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMin.append(ability_min[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMin.append(ability_min[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMin.append(ability_min[\"lavaAbility\"])\n",
        "  rightAbilityAllMin.append(ability_min[\"rightAbility\"])\n",
        "  leftAbilityAllMin.append(ability_min[\"leftAbility\"])\n",
        "  aheadAbilityAllMin.append(ability_min[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllMax.append(ability_max[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMax.append(ability_max[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMax.append(ability_max[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMax.append(ability_max[\"lavaAbility\"])\n",
        "  rightAbilityAllMax.append(ability_max[\"rightAbility\"])\n",
        "  leftAbilityAllMax.append(ability_max[\"leftAbility\"])\n",
        "  aheadAbilityAllMax.append(ability_max[\"aheadAbility\"])\n",
        "\n",
        "  meanSuccess.append(np.mean(agent_data[f\"success_{agent}\"]))\n",
        "  meanChoice.append(np.mean(agent_data[f\"correctChoice_{agent}\"]))\n",
        "\n",
        "  agentName.append(agent)\n",
        "\n",
        "  finalDF = pd.DataFrame({\"Agent Name\": agentName,\n",
        "                          \"Model Brier Score Success\":modelBrierScoreSuccess,\n",
        "                          \"Model Brier Score Choice\":modelBrierScoreChoice,\n",
        "                          \"Aggregate Brier Score Success\": aggregateBrierScoreSuccess,\n",
        "                          \"Aggregate Brier Score Choice\": aggregateBrierScoreChoice,\n",
        "                          \"Model Calibration Success\": modelCalibrationSuccess,\n",
        "                          \"Model Calibration Choice\": modelCalibrationChoice,\n",
        "                          \"Aggregate Calibration Success\":aggregateCalibrationSuccess,\n",
        "                          \"Aggregate Calibration Choice\":aggregateCalibrationChoice,\n",
        "                          \"Model Refinement Success\" : modelRefinementSuccess,\n",
        "                          \"Model Refinement Choice\" : modelRefinementChoice,\n",
        "                          \"Aggregate Refinement Success\" : aggregateRefinementSuccess,\n",
        "                          \"Aggregate Refinement Choice\" : aggregateRefinementChoice,\n",
        "                          \"Model Better? (Based on Brier Score - Success)\":modelBetterThanAggSuccess,\n",
        "                          \"Model Better? (Based on Brier Score - Choice)\":modelBetterThanAggChoice,\n",
        "                          \"Average Success\":meanSuccess,\n",
        "                          \"Average Correct Chocie\":meanChoice,\n",
        "                          \"Object Permanence Ability Mean (All)\": OPAbilityAllMean,\n",
        "                          \"Visual Acuity Ability Mean (All)\" : visualAcuityAbilityAllMean,\n",
        "                          \"Flat Navigation Ability Mean (All)\" : flatNavAbilityAllMean,\n",
        "                          \"Lava Ability Mean (All)\" : lavaAbilityAllMean,\n",
        "                          \"Right Ability Mean (All)\" : rightAbilityAllMean,\n",
        "                          \"Left Ability Mean (All)\" : leftAbilityAllMean,\n",
        "                          \"Ahead Ability Mean (All)\" : aheadAbilityAllMean,\n",
        "                          \"Object Permanence Ability SD (All)\": OPAbilityAllSD,\n",
        "                          \"Visual Acuity Ability SD (All)\" : visualAcuityAbilityAllSD,\n",
        "                          \"Flat Navigation Ability SD (All)\" : flatNavAbilityAllSD,\n",
        "                          \"Lava Ability SD (All)\" : lavaAbilityAllSD,\n",
        "                          \"Right Ability SD (All)\" : rightAbilityAllSD,\n",
        "                          \"Left Ability SD (All)\" : leftAbilityAllSD,\n",
        "                          \"Ahead Ability SD (All)\" : aheadAbilityAllSD,\n",
        "                          \"Object Permanence Ability Min (All)\": OPAbilityAllMin,\n",
        "                          \"Visual Acuity Ability Min (All)\" : visualAcuityAbilityAllMin,\n",
        "                          \"Flat Navigation Ability Min (All)\" : flatNavAbilityAllMin,\n",
        "                          \"Lava Ability Min (All)\" : lavaAbilityAllMin,\n",
        "                          \"Right Ability Min (All)\" : rightAbilityAllMin,\n",
        "                          \"Left Ability Min (All)\" : leftAbilityAllMin,\n",
        "                          \"Ahead Ability Min (All)\" : aheadAbilityAllMin,\n",
        "                          \"Object Permanence Ability Max (All)\": OPAbilityAllMax,\n",
        "                          \"Visual Acuity Ability Max (All)\" : visualAcuityAbilityAllMax,\n",
        "                          \"Flat Navigation Ability Max (All)\" : flatNavAbilityAllMax,\n",
        "                          \"Lava Ability Max (All)\" : lavaAbilityAllMax,\n",
        "                          \"Right Ability Max (All)\" : rightAbilityAllMax,\n",
        "                          \"Left Ability Max (All)\" : leftAbilityAllMax,\n",
        "                          \"Ahead Ability Max (All)\" : aheadAbilityAllMax,})\n",
        "  finalDF.to_csv(\"/content/drive/Shareddrives/comparative-object-permanence/measurement_layout_results/AgentResults.csv\", index = False)\n",
        "\n",
        "  ## figures\n",
        "\n",
        "  for ability in abilitiesToShow:\n",
        "    trace_plot = az.plot_trace(data=data_all['posterior'][[ability]])\n",
        "    plt.savefig(f\"{figures_folder}/traceplot_{agent}_{ability}.png\")\n",
        "\n",
        "    forest_plot = az.plot_forest(data=data_all['posterior'][[ability]])\n",
        "    plt.savefig(f\"{figures_folder}/forest_{agent}_{ability}.png\")\n",
        "\n",
        "  energy_plot = az.plot_energy(data=data_all)\n",
        "  plt.savefig(f\"{figures_folder}/energyplot_{agent}.png\")\n",
        "\n",
        "  summary = az.summary(data_all['posterior'][abilitiesToShow])\n",
        "  save_object(summary, f\"{full_data_model_folder}/summary_{agent}.pkl\")\n",
        "  summary.to_csv(f\"{full_data_model_folder}/summary_{agent}.csv\", index = False)\n",
        "\n",
        "  print(f\"Agent: {agent}\")\n",
        "  print(summary)\n"
      ],
      "metadata": {
        "id": "MVneimSHDeIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for agent in reference_agents_types:\n",
        "  agent_subset = long_data[(long_data['agent_type_gen'].str.contains(agent))].dropna(subset = ['success', 'correctChoice'])\n",
        "  train_data, test_data = train_test_split(agent_subset, test_size = 0.2)\n",
        "\n",
        "  model_train, ability_min, ability_max = SetupMultivariateHierarchicalModel(train_data, uniformAbilitySlack=1)\n",
        "  with model_train:\n",
        "    data_training = pm.sample(pymc_sample_num, target_accept=0.95)\n",
        "\n",
        "  #save_object(model_train, f\"{training_data_model_folder}/model_{agent}.pkl\") #save the model just in case we need it again.\n",
        "\n",
        "  predictionsSuccessInstance, predictionsChoiceInstance, successes, choices = predict(model_train, data_training, test_data, \"success\", \"correctChoice\", len(train_data), multi_var=True)\n",
        "\n",
        "  agentBrierScoreSuccess, agentCalibrationSuccess, agentRefinementSuccess = brierDecomp(predictionsSuccessInstance, successes)\n",
        "  agentAggBrierScoreSuccess, agentAggCalibrationSuccess, agentAggRefinementSuccess = brierDecomp(np.repeat(np.mean(train_data[f\"{agent}_success\"]), len(successes)), successes)\n",
        "  agentBrierScoreChoice, agentCalibrationChoice, agentRefinementChoice = brierDecomp(predictionsChoiceInstance, choices)\n",
        "  agentAggBrierScoreChoice, agentAggCalibrationChoice, agentAggRefinementChoice = brierDecomp(np.repeat(np.mean(train_data[f\"{agent}_choice\"]), len(choices)), choices)\n",
        "\n",
        "  aggregateBrierScoreSuccess.append(agentAggBrierScoreSuccess)\n",
        "  aggregateBrierScoreChoice.append(agentAggBrierScoreChoice)\n",
        "  aggregateCalibrationSuccess.append(agentAggCalibrationSuccess)\n",
        "  aggregateCalibrationChoice.append(agentAggCalibrationChoice)\n",
        "  aggregateRefinementSuccess.append(agentAggRefinementSuccess)\n",
        "  aggregateRefinementChoice.append(agentAggRefinementChoice)\n",
        "\n",
        "  modelBrierScoreSuccess.append(agentBrierScoreSuccess)\n",
        "  modelBrierScoreChoice.append(agentBrierScoreChoice)\n",
        "  modelCalibrationSuccess.append(agentCalibrationSuccess)\n",
        "  modelCalibrationChoice.append(agentCalibrationChoice)\n",
        "  modelRefinementSuccess.append(agentRefinementSuccess)\n",
        "  modelRefinementChoice.append(agentRefinementChoice)\n",
        "\n",
        "  if agentBrierScoreSuccess < agentAggBrierScoreSuccess:\n",
        "    modelBetterThanAggSuccess.append(True)\n",
        "  else:\n",
        "    modelBetterThanAggSuccess.append(False)\n",
        "\n",
        "  if agentBrierScoreChoice < agentAggBrierScoreChoice:\n",
        "    modelBetterThanAggChoice.append(True)\n",
        "  else:\n",
        "    modelBetterThanAggChoice.append(False)\n",
        "\n",
        "  # now train on all data\n",
        "\n",
        "  model_all, ability_min, ability_max = SetupMultivariateHierarchicalModel(agent_subset, uniformAbilitySlack=1)\n",
        "  with model_all:\n",
        "    data_all = pm.sample(pymc_sample_num, target_accept=0.95)\n",
        "\n",
        "  #save_object(data_all, f\"{full_data_model_folder}/model_{agent}.pkl\") #save the model just in case we need it again.\n",
        "\n",
        "  mu, sd  = analyzeAgentResults(data_all, abilitiesToShow)\n",
        "\n",
        "  OPAbilityAllMean.append(mu[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMean.append(mu[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMean.append(mu[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMean.append(mu[\"lavaAbility\"])\n",
        "  rightAbilityAllMean.append(mu[\"rightAbility\"])\n",
        "  leftAbilityAllMean.append(mu[\"leftAbility\"])\n",
        "  aheadAbilityAllMean.append(mu[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllSD.append(sd[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllSD.append(sd[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllSD.append(sd[\"flatNavAbility\"])\n",
        "  lavaAbilityAllSD.append(sd[\"lavaAbility\"])\n",
        "  rightAbilityAllSD.append(sd[\"rightAbility\"])\n",
        "  leftAbilityAllSD.append(sd[\"leftAbility\"])\n",
        "  aheadAbilityAllSD.append(sd[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllMin.append(ability_min[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMin.append(ability_min[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMin.append(ability_min[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMin.append(ability_min[\"lavaAbility\"])\n",
        "  rightAbilityAllMin.append(ability_min[\"rightAbility\"])\n",
        "  leftAbilityAllMin.append(ability_min[\"leftAbility\"])\n",
        "  aheadAbilityAllMin.append(ability_min[\"aheadAbility\"])\n",
        "\n",
        "  OPAbilityAllMax.append(ability_max[\"objPermAbility\"])\n",
        "  visualAcuityAbilityAllMax.append(ability_max[\"visualAcuityAbility\"])\n",
        "  flatNavAbilityAllMax.append(ability_max[\"flatNavAbility\"])\n",
        "  lavaAbilityAllMax.append(ability_max[\"lavaAbility\"])\n",
        "  rightAbilityAllMax.append(ability_max[\"rightAbility\"])\n",
        "  leftAbilityAllMax.append(ability_max[\"leftAbility\"])\n",
        "  aheadAbilityAllMax.append(ability_max[\"aheadAbility\"])\n",
        "\n",
        "  meanSuccess.append(np.mean(agent_subset['success']))\n",
        "  meanChoice.append(np.mean(agent_subset['correctChoice']))\n",
        "\n",
        "  agentName.append(agent)\n",
        "\n",
        "  finalDF = pd.DataFrame({\"Agent Name\": agentName,\n",
        "                          \"Model Brier Score Success\":modelBrierScoreSuccess,\n",
        "                          \"Model Brier Score Choice\":modelBrierScoreChoice,\n",
        "                          \"Aggregate Brier Score Success\": aggregateBrierScoreSuccess,\n",
        "                          \"Aggregate Brier Score Choice\": aggregateBrierScoreChoice,\n",
        "                          \"Model Calibration Success\": modelCalibrationSuccess,\n",
        "                          \"Model Calibration Choice\": modelCalibrationChoice,\n",
        "                          \"Aggregate Calibration Success\":aggregateCalibrationSuccess,\n",
        "                          \"Aggregate Calibration Choice\":aggregateCalibrationChoice,\n",
        "                          \"Model Refinement Success\" : modelRefinementSuccess,\n",
        "                          \"Model Refinement Choice\" : modelRefinementChoice,\n",
        "                          \"Aggregate Refinement Success\" : aggregateRefinementSuccess,\n",
        "                          \"Aggregate Refinement Choice\" : aggregateRefinementChoice,\n",
        "                          \"Model Better? (Based on Brier Score - Success)\":modelBetterThanAggSuccess,\n",
        "                          \"Model Better? (Based on Brier Score - Choice)\":modelBetterThanAggChoice,\n",
        "                          \"Average Success\":meanSuccess,\n",
        "                          \"Average Correct Chocie\":meanChoice,\n",
        "                          \"Object Permanence Ability Mean (All)\": OPAbilityAllMean,\n",
        "                          \"Visual Acuity Ability Mean (All)\" : visualAcuityAbilityAllMean,\n",
        "                          \"Flat Navigation Ability Mean (All)\" : flatNavAbilityAllMean,\n",
        "                          \"Lava Ability Mean (All)\" : lavaAbilityAllMean,\n",
        "                          \"Right Ability Mean (All)\" : rightAbilityAllMean,\n",
        "                          \"Left Ability Mean (All)\" : leftAbilityAllMean,\n",
        "                          \"Ahead Ability Mean (All)\" : aheadAbilityAllMean,\n",
        "                          \"Object Permanence Ability SD (All)\": OPAbilityAllSD,\n",
        "                          \"Visual Acuity Ability SD (All)\" : visualAcuityAbilityAllSD,\n",
        "                          \"Flat Navigation Ability SD (All)\" : flatNavAbilityAllSD,\n",
        "                          \"Lava Ability SD (All)\" : lavaAbilityAllSD,\n",
        "                          \"Right Ability SD (All)\" : rightAbilityAllSD,\n",
        "                          \"Left Ability SD (All)\" : leftAbilityAllSD,\n",
        "                          \"Ahead Ability SD (All)\" : aheadAbilityAllSD,\n",
        "                          \"Object Permanence Ability Min (All)\": OPAbilityAllMin,\n",
        "                          \"Visual Acuity Ability Min (All)\" : visualAcuityAbilityAllMin,\n",
        "                          \"Flat Navigation Ability Min (All)\" : flatNavAbilityAllMin,\n",
        "                          \"Lava Ability Min (All)\" : lavaAbilityAllMin,\n",
        "                          \"Right Ability Min (All)\" : rightAbilityAllMin,\n",
        "                          \"Left Ability Min (All)\" : leftAbilityAllMin,\n",
        "                          \"Ahead Ability Min (All)\" : aheadAbilityAllMin,\n",
        "                          \"Object Permanence Ability Max (All)\": OPAbilityAllMax,\n",
        "                          \"Visual Acuity Ability Max (All)\" : visualAcuityAbilityAllMax,\n",
        "                          \"Flat Navigation Ability Max (All)\" : flatNavAbilityAllMax,\n",
        "                          \"Lava Ability Max (All)\" : lavaAbilityAllMax,\n",
        "                          \"Right Ability Max (All)\" : rightAbilityAllMax,\n",
        "                          \"Left Ability Max (All)\" : leftAbilityAllMax,\n",
        "                          \"Ahead Ability Max (All)\" : aheadAbilityAllMax,})\n",
        "  finalDF.to_csv(\"/content/drive/Shareddrives/comparative-object-permanence/measurement_layout_results/AgentResults.csv\", index = False)\n",
        "\n",
        "  ## figures\n",
        "\n",
        "  for ability in abilitiesToShow:\n",
        "    trace_plot = az.plot_trace(data=data_all['posterior'][[ability]])\n",
        "    plt.savefig(f\"{figures_folder}/traceplot_{agent}_{ability}.png\")\n",
        "\n",
        "    forest_plot = az.plot_forest(data=data_all['posterior'][[ability]])\n",
        "    plt.savefig(f\"{figures_folder}/forest_{agent}_{ability}.png\")\n",
        "\n",
        "  energy_plot = az.plot_energy(data=data_all)\n",
        "  plt.savefig(f\"{figures_folder}/energyplot_{agent}.png\")\n",
        "\n",
        "  summary = az.summary(data_all['posterior'][abilitiesToShow])\n",
        "  save_object(summary, f\"{full_data_model_folder}/summary_{agent}.pkl\")\n",
        "  summary.to_csv(f\"{full_data_model_folder}/summary_{agent}.csv\", index = False)\n",
        "\n",
        "  print(f\"Agent: {agent}\")\n",
        "  print(summary)\n"
      ],
      "metadata": {
        "id": "cv20hESCFnZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}