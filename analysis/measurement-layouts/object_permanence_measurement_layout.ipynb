{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kinds-of-Intelligence-CFI/comparative-object-permanence/blob/measurement-layout/analysis/measurement-layouts/object_permanence_measurement_layout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparative Object Permanence Measurement Layouts\n",
        "\n",
        "Authors: K. Voudouris, J. Burden, J. Hern√°ndez-Orallo"
      ],
      "metadata": {
        "id": "U27yVRKBqCpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INIT"
      ],
      "metadata": {
        "id": "95RSQ17Lriav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLYI9s6Lp3fi"
      },
      "outputs": [],
      "source": [
        "!pip install pymc --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip install arviz --quiet\n",
        "!pip install erroranalysis --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import erroranalysis as ea\n",
        "import gc\n",
        "import graphviz\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import random as rm\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\")"
      ],
      "metadata": {
        "id": "ehpymOwzrgm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "s5Nsn4LgzVmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agents_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/measurement-layout/analysis/measurement-layouts/results_final_clean_agents_wide.csv?token=GHSAT0AAAAAACEGARGQY7IJDAXM63HIYWZIZG75C4A'\n",
        "agent_data = pd.read_csv(agents_url)\n",
        "\n",
        "children_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/measurement-layout/analysis/measurement-layouts/results_final_clean_children_wide.csv?token=GHSAT0AAAAAACEGARGQXTFZR34LOMM4BZIOZG75DGA'\n",
        "children_data = pd.read_csv(children_url)\n",
        "\n",
        "synthetic_agents_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/comparative-object-permanence/measurement-layout/analysis/measurement-layouts/results_synthetic_agents_wide.csv?token=GHSAT0AAAAAACEGARGQWE57GWCYBGAO756CZG75DRA'\n",
        "synthetic_agents_data = pd.read_csv(synthetic_agents_url)\n"
      ],
      "metadata": {
        "id": "1dN0OHAWzVBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Measurement Layout"
      ],
      "metadata": {
        "id": "C0_GsfhIYZPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper functions\n",
        "\n",
        "def logistic(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def visualAcuityLOMargin(ability, goalSmallness): # must return a value between -inf and inf  (more precisely between -maxVisualAcuityAbility and maxVisualAcuityAbility)\n",
        "  return ability - goalSmallness   # Goes between -inf to inf, with logodds=0 meaning this would lead to 0.5 chance of success\n",
        "\n",
        "def SimplePrMargin(ability, binaryFeature): # must return a value between 0 and 1\n",
        "  return 1-((1-ability)*binaryFeature)  # If binaryFeature is 0 then the margin represents p(success)=1. If binaryFeature = 1 then p(success)=ability\n",
        "\n",
        "#def flatNavMargin(ability, distanceToGoal, numTurns, allocentricOcclusion): # must return a value between -inf and inf  (more precisely between -maxSpatialAbility and maxSpatialAbility)\n",
        "def flatNavMargin(ability, distanceToGoal, numTurns):\n",
        "  #return ability - ((distanceToGoal * numTurns)*allocentricOcclusion)   # Goes between -inf to inf, with logodds=0 meaning this would lead to 0.5 chance of success\n",
        "  return ability - ((distanceToGoal * numTurns))\n",
        "\n",
        "def objPermanenceLOMargin(ability, allocentricOcclusion, cvChickP, pctbP, lightsOut, distanceToGoal, numPositions, maxPermAbility, lightsOutPenalisation, uniformAbilitySlack): #,  visualAcuityP): # must return a value between -inf and inf (more precisely between -maxPermAbility and maxPermAbility)\n",
        "#def objPermanenceLOMargin(ability, allocentricOcclusion, lightsOut, distanceToGoal, numPositions, maxPermAbility, lightsOutPenalisation, uniformAbilitySlack):\n",
        "  assert lightsOutPenalisation < uniformAbilitySlack and lightsOutPenalisation > 1, \"Penalisation for lights out must be lower than the slack on abilities.\"\n",
        "  OPPerformance = ability  - ((distanceToGoal * numPositions)*(lightsOut*lightsOutPenalisation)) #this returns a value that is lower when (a) ability is lower, (b) the goal is occluded for longer or there are more positions where it could be occluded, or (c) when the lights go out (by a small penalisation)\n",
        "  ability = maxPermAbility - ((maxPermAbility-OPPerformance)*allocentricOcclusion) * cvChickP * pctbP # this returns an ability value that is modulated by performance on the different paradigms. The agent needs to be good at both to have a high OP ability\n",
        "  #return maxPermAbility-((maxPermAbility-OPPerformance)*((allocentricOcclusion*cvChick) + (allocentricOcclusion*pctb))) #multiply by a factor that introduces whether the instance is a an OP CV task or an OP PCTB task (independent, so ((allocentricOcclusion*cvChick) + (allocentricOcclusion*pctb)) should be 0 or 1)\n",
        "  #return maxPermAbility - ((maxPermAbility-OPPerformance)*allocentricOcclusion)\n",
        "  return ability\n"
      ],
      "metadata": {
        "id": "-uobQpcJiPr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SetupModel(taskResultsAll, uniformAbilitySlack, agent_type, agent_name = None, sample = 500):\n",
        "  \"\"\"\n",
        "  taskResults is the conjunction of the metadata with the successes of the agents on that set of tests.\n",
        "  \"\"\"\n",
        "  assert uniformAbilitySlack >=1, \"Slack must be greater than or equal to 1.\"\n",
        "\n",
        "  if agent_type == \"agent\":\n",
        "    taskResults = taskResultsAll.dropna(subset = [agent_name])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults[agent_name]\n",
        "  elif agent_type == \"child\":\n",
        "    taskResults = taskResultsAll.dropna(subset = ['success'])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults['success']\n",
        "  else:\n",
        "    print(\"Agent not recognised. Quitting.\")\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  abilityMin = {} #Initialize ability Min and max dataframes for plotting ranges\n",
        "  abilityMax = {}\n",
        "\n",
        "  #Decide \"maximum capabilities\" based on the hardest values in the dataset\n",
        "  maxDistance = taskResults[\"cityBlockDistanceToGoal\"].max()\n",
        "  maxTurns = taskResults[\"minNumTurnsRequired\"].max()\n",
        "  maxChoices = taskResults[\"numChoices\"].max()\n",
        "  maxGoalSize = taskResults[\"mainGoalSize\"].max()\n",
        "  maxPermAbility = maxDistance * maxChoices * uniformAbilitySlack\n",
        "  #maxMemoryAbility = maxDistance * uniformAbilitySlack\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", 0, maxPermAbility)  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "    abilityMin[\"objPermAbility\"] = 0\n",
        "    abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "    #memoryAbility = pm.Uniform(\"memoryAbility\", 0, maxMemoryAbility)  # [0,maxMemoryAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "    #abilityMin[\"memoryAbility\"] = 0\n",
        "    #abilityMax[\"memoryAbility\"] = maxMemoryAbility\n",
        "\n",
        "    #Visual acuity\n",
        "    maxVisualAcuityAbility = maxGoalSize* uniformAbilitySlack\n",
        "    visualAcuityAbility = pm.Uniform(\"visualAcuityAbility\", 0, maxVisualAcuityAbility)\n",
        "    abilityMin[\"visualAcuityAbility\"] = 0\n",
        "    abilityMax[\"visualAcuityAbility\"] = maxVisualAcuityAbility\n",
        "\n",
        "    # Flat Navigation Ability\n",
        "    flatNavAbility = pm.Uniform(\"flatNavAbility\", 0, maxDistance*maxTurns)      # how much navigation is involved, i.e, how far away and how circuitous is the path to the goal?\n",
        "    abilityMin[\"flatNavAbility\"] =0\n",
        "    abilityMax[\"flatNavAbility\"] = maxDistance*maxTurns\n",
        "\n",
        "    # Lava Ability\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1,1)                        # [0,1] Specific ability with lava\n",
        "    abilityMin[\"lavaAbility\"] = 0\n",
        "    abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "    # Ramp Ability\n",
        "    rampAbility = pm.Beta(\"rampAbility\",1,1)                          # [0,1] Specific ability with ramps\n",
        "    abilityMin[\"rampAbility\"] = 0\n",
        "    abilityMax[\"rampAbility\"] = 1\n",
        "\n",
        "    # Goal Right Ability\n",
        "    rightAbility = pm.Beta(\"rightAbility\", 1, 1)\n",
        "    abilityMin[\"rightAbility\"] = 0\n",
        "    abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "    # Goal Left Ability\n",
        "    leftAbility = pm.Beta(\"leftAbility\", 1, 1)\n",
        "    abilityMin[\"leftAbility\"] = 0\n",
        "    abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "    # Goal Ahead Ability\n",
        "    aheadAbility = pm.Beta(\"aheadAbility\", 1, 1)\n",
        "    abilityMin[\"aheadAbility\"] = 0\n",
        "    abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "    # CV Chick Ability\n",
        "    CVChickAbility = pm.Beta(\"CVChickAbility\", 1, 1)\n",
        "    abilityMin[\"CVChickAbility\"] = 0\n",
        "    abilityMax[\"CVChickAbility\"] = 1\n",
        "\n",
        "    # PCTB Ability\n",
        "    PCTBAbility = pm.Beta(\"PCTBAbility\", 1, 1)\n",
        "    abilityMin[\"PCTBAbility\"] = 0\n",
        "    abilityMax[\"PCTBAbility\"] = 1\n",
        "\n",
        "\n",
        "\n",
        "    ## Environment variables as Deterministic (about the instance)\n",
        "\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", taskResults[\"lavaPresence\"].values)\n",
        "    rampPresence = pm.MutableData(\"rampPresence\", taskResults[\"taskCriticalRampPresence\"].values)\n",
        "    lightsOutPresence = pm.MutableData(\"lightsOutPresence\", taskResults[\"lightsOutPresence\"].values)\n",
        "    #numGoals = pm.MutableData(\"numberOfGoals\", taskResults[\"numGoalsAll\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", taskResults[\"numChoices\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\",taskResults[\"mainGoalSize\"].values)\n",
        "    goalDist = pm.MutableData(\"goalDistance\", taskResults[\"cityBlockDistanceToGoal\"])\n",
        "    numTurns = pm.MutableData(\"minTurnsToGoal\", taskResults[\"minNumTurnsRequired\"])\n",
        "    goalRight = pm.MutableData(\"goalRight\", taskResults[\"goalRightRelToStart\"])\n",
        "    goalAhead = pm.MutableData(\"goalAhead\", taskResults[\"goalCentreRelToStart\"])\n",
        "    goalLeft = pm.MutableData(\"goalLeft\", taskResults[\"goalLeftRelToStart\"])\n",
        "    opTest = pm.MutableData(\"allocentricOPTest\", taskResults[\"goalBecomesAllocentricallyOccluded\"])\n",
        "    CVTest = pm.MutableData(\"CVChickTest\", taskResults[\"cvchickTask\"])\n",
        "    PCTBTest = pm.MutableData(\"PCTBTest\", taskResults[\"pctbTask\"])\n",
        "\n",
        "    ## Margins\n",
        "\n",
        "    goalSmallness = maxGoalSize - goalSize\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic(visualAcuityLOMargin(visualAcuityAbility, goalSmallness)))\n",
        "\n",
        "    rightP = pm.Deterministic(\"rightPerformance\", SimplePrMargin(rightAbility, goalRight))\n",
        "    aheadP = pm.Deterministic(\"aheadPerformance\", SimplePrMargin(aheadAbility, goalAhead))\n",
        "    leftP = pm.Deterministic(\"leftPerformance\", SimplePrMargin(leftAbility, goalLeft))\n",
        "\n",
        "    #flatNavP = pm.Deterministic(\"flatNavP\", logistic(flatNavMargin(flatNavAbility, goalDist, numTurns, opTest)))\n",
        "    flatNavP = pm.Deterministic(\"flatNavP\", logistic(flatNavMargin(flatNavAbility, goalDist, numTurns)))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", SimplePrMargin(lavaAbility, lavaPresence))\n",
        "\n",
        "    rampP = pm.Deterministic(\"rampP\", SimplePrMargin(rampAbility, rampPresence))\n",
        "\n",
        "    cvchickP = pm.Deterministic(\"cvChickP\", SimplePrMargin(CVChickAbility, CVTest))\n",
        "    pctbP = pm.Deterministic(\"pctbP\", SimplePrMargin(PCTBAbility, PCTBTest))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", (flatNavP * lavaP * rampP * rightP * aheadP * leftP * cvchickP * pctbP)) #, visualAcuityP))\n",
        "\n",
        "    #OPLOM = objPermanenceLOMargin(objPermAbility, opTest, CVTest, PCTBTest, lightsOutPresence, goalDist, numChoices, maxPermAbility, lightsOutPenalisation=(uniformAbilitySlack-0.35), uniformAbilitySlack=uniformAbilitySlack)\n",
        "    #OPLOM = objPermanenceLOMargin(objPermAbility, opTest, lightsOutPresence, goalDist, numChoices, maxPermAbility, lightsOutPenalisation=(uniformAbilitySlack-0.35), uniformAbilitySlack=uniformAbilitySlack)\n",
        "    OPLOM = objPermanenceLOMargin(objPermAbility, opTest, cvchickP, pctbP, lightsOutPresence, goalDist, numChoices, maxPermAbility, lightsOutPenalisation=(uniformAbilitySlack-0.35), uniformAbilitySlack=uniformAbilitySlack)\n",
        "\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic(OPLOM))\n",
        "\n",
        "    noise = 1 - np.mean(results)  # With this noise is complementary to result prior.\n",
        "    noisePar = pm.Uniform(\"noisePar\", 0, 1)\n",
        "    finalP = pm.Deterministic(\"finalP\", (1-noisePar)*(objPermP * navP * visualAcuityP)+(noisePar*noise))\n",
        "\n",
        "    #finalP = pm.Deterministic(\"finalP\", (objPermP * navP * visualAcuityP))\n",
        "\n",
        "    taskPerformance = pm.Bernoulli(\"taskPerformance\", finalP, observed=results)\n",
        "\n",
        "  pm.model_graph.model_to_graphviz(m)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "9JrHfqlDYYwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, min, max = SetupModel(synthetic_agents_data, uniformAbilitySlack=1.4, agent_type='agent', agent_name=\"perfectAgent\", sample = None)\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv.format=\"png\"\n",
        "gv.render\n",
        "#gv.render(directory='viz')\n",
        "#Image(\"viz/Digraph.gv.png\")\n",
        "gv"
      ],
      "metadata": {
        "id": "6ZxN1nZJm5oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SetupModelSimple(taskResultsAll, uniformAbilitySlack, agent_type, agent_name = None, sample = 500):\n",
        "  \"\"\"\n",
        "  taskResults is the conjunction of the metadata with the successes of the agents on that set of tests.\n",
        "  \"\"\"\n",
        "  assert uniformAbilitySlack >=1, \"Slack must be greater than or equal to 1.\"\n",
        "\n",
        "  if agent_type == \"agent\":\n",
        "    taskResults = taskResultsAll.dropna(subset = [agent_name])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults[agent_name]\n",
        "  elif agent_type == \"child\":\n",
        "    taskResults = taskResultsAll.dropna(subset = ['success'])\n",
        "    if sample is not None:\n",
        "      taskResults = taskResults.sample(n=sample)\n",
        "    results = taskResults['success']\n",
        "  else:\n",
        "    print(\"Agent not recognised. Quitting.\")\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "  abilityMin = {} #Initialize ability Min and max dataframes for plotting ranges\n",
        "  abilityMax = {}\n",
        "\n",
        "  #Decide \"maximum capabilities\" based on the hardest values in the dataset\n",
        "  maxDistance = taskResults[\"cityBlockDistanceToGoal\"].max()\n",
        "  maxTurns = taskResults[\"minNumTurnsRequired\"].max()\n",
        "  maxChoices = taskResults[\"numChoices\"].max()\n",
        "  maxGoalSize = taskResults[\"mainGoalSize\"].max()\n",
        "  maxPermAbility = ((taskResults[\"cityBlockDistanceToGoal\"] * taskResults[\"numChoices\"]).max()) * uniformAbilitySlack\n",
        "  maxFlatNav = ((taskResults[\"cityBlockDistanceToGoal\"]*taskResults[\"minNumTurnsRequired\"])).max() *uniformAbilitySlack\n",
        "  #maxPermAbility = maxChoices * uniformAbilitySlack\n",
        "  #maxMemoryAbility = maxDistance * uniformAbilitySlack\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", 0, maxPermAbility)  # [0,maxPermAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "    abilityMin[\"objPermAbility\"] = 0\n",
        "    abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "    #memoryAbility = pm.Uniform(\"memoryAbility\", 0, maxMemoryAbility)  # [0,maxMemoryAbility] This is the same as above, but we have the ability in the right magnitude\n",
        "    #abilityMin[\"memoryAbility\"] = 0\n",
        "    #abilityMax[\"memoryAbility\"] = maxMemoryAbility\n",
        "\n",
        "    #Visual acuity\n",
        "    #maxVisualAcuityAbility = maxGoalSize* uniformAbilitySlack\n",
        "    #visualAcuityAbility = pm.Uniform(\"visualAcuityAbility\", 0, maxVisualAcuityAbility)\n",
        "    #abilityMin[\"visualAcuityAbility\"] = 0\n",
        "    #abilityMax[\"visualAcuityAbility\"] = maxVisualAcuityAbility\n",
        "\n",
        "    # Flat Navigation Ability\n",
        "    #flatNavAbility = pm.Uniform(\"flatNavAbility\", 0, maxDistance*maxTurns)      # how much navigation is involved, i.e, how far away and how circuitous is the path to the goal?\n",
        "    #flatNavAbility = pm.Uniform(\"flatNavAbility\", 0, maxTurns*maxDistance)\n",
        "    #abilityMin[\"flatNavAbility\"] = 0\n",
        "    #abilityMax[\"flatNavAbility\"] = maxDistance*maxTurns\n",
        "    #abilityMax[\"flatNavAbility\"] = maxFlatNav\n",
        "\n",
        "    # Lava Ability\n",
        "    #lavaAbility = pm.Beta(\"lavaAbility\", 1,1)                        # [0,1] Specific ability with lava\n",
        "    #abilityMin[\"lavaAbility\"] = 0\n",
        "    #abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "    # Ramp Ability\n",
        "    #rampAbility = pm.Beta(\"rampAbility\",1,1)                          # [0,1] Specific ability with ramps\n",
        "    #abilityMin[\"rampAbility\"] = 0\n",
        "    #abilityMax[\"rampAbility\"] = 1\n",
        "\n",
        "    # Goal Right Ability\n",
        "    #rightAbility = pm.Beta(\"rightAbility\", 1, 1)\n",
        "    #abilityMin[\"rightAbility\"] = 0\n",
        "    #abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "    # Goal Left Ability\n",
        "    #leftAbility = pm.Beta(\"leftAbility\", 1, 1)\n",
        "    #abilityMin[\"leftAbility\"] = 0\n",
        "    #abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "    # Goal Ahead Ability\n",
        "    #aheadAbility = pm.Beta(\"aheadAbility\", 1, 1)\n",
        "    #abilityMin[\"aheadAbility\"] = 0\n",
        "    #abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "    # CV Chick Ability\n",
        "    #CVChickAbility = pm.Beta(\"CVChickAbility\", 1, 1)\n",
        "    #abilityMin[\"CVChickAbility\"] = 0\n",
        "    #abilityMax[\"CVChickAbility\"] = 1\n",
        "\n",
        "    # PCTB Ability\n",
        "    #PCTBAbility = pm.Beta(\"PCTBAbility\", 1, 1)\n",
        "    #abilityMin[\"PCTBAbility\"] = 0\n",
        "    #abilityMax[\"PCTBAbility\"] = 1\n",
        "\n",
        "\n",
        "\n",
        "    ## Environment variables as Deterministic (about the instance)\n",
        "\n",
        "    #lavaPresence = pm.MutableData(\"lavaPresence\", taskResults[\"lavaPresence\"].values)\n",
        "    #rampPresence = pm.MutableData(\"rampPresence\", taskResults[\"taskCriticalRampPresence\"].values)\n",
        "    #lightsOutPresence = pm.MutableData(\"lightsOutPresence\", taskResults[\"lightsOutPresence\"].values)\n",
        "    #numGoals = pm.MutableData(\"numberOfGoals\", taskResults[\"numGoalsAll\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", taskResults[\"numChoices\"].values)\n",
        "    #goalSize = pm.MutableData(\"goalSize\",taskResults[\"mainGoalSize\"].values)\n",
        "    goalDist = pm.MutableData(\"goalDistance\", taskResults[\"cityBlockDistanceToGoal\"])\n",
        "    #numTurns = pm.MutableData(\"minTurnsToGoal\", taskResults[\"minNumTurnsRequired\"])\n",
        "    #goalRight = pm.MutableData(\"goalRight\", taskResults[\"goalRightRelToStart\"])\n",
        "    #goalAhead = pm.MutableData(\"goalAhead\", taskResults[\"goalCentreRelToStart\"])\n",
        "    #goalLeft = pm.MutableData(\"goalLeft\", taskResults[\"goalLeftRelToStart\"])\n",
        "    opTest = pm.MutableData(\"allocentricOPTest\", taskResults[\"goalBecomesAllocentricallyOccluded\"])\n",
        "    #CVTest = pm.MutableData(\"CVChickTest\", taskResults[\"cvchickTask\"])\n",
        "    #PCTBTest = pm.MutableData(\"PCTBTest\", taskResults[\"pctbTask\"])\n",
        "\n",
        "    ## Margins\n",
        "\n",
        "    #goalSmallness = maxGoalSize - goalSize\n",
        "    #visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic(visualAcuityLOMargin(visualAcuityAbility, goalSmallness)))\n",
        "\n",
        "    #rightP = pm.Deterministic(\"rightPerformance\", SimplePrMargin(rightAbility, goalRight))\n",
        "    #aheadP = pm.Deterministic(\"aheadPerformance\", SimplePrMargin(aheadAbility, goalAhead))\n",
        "    #leftP = pm.Deterministic(\"leftPerformance\", SimplePrMargin(leftAbility, goalLeft))\n",
        "\n",
        "    #flatNavP = pm.Deterministic(\"flatNavP\", logistic(flatNavMargin(flatNavAbility, goalDist, numTurns, opTest)))\n",
        "    #flatNavP = pm.Deterministic(\"flatNavP\", logistic(flatNavMargin(flatNavAbility, goalDist, numTurns)))\n",
        "    #flatNavP = pm.Deterministic(\"flatNavP\", logistic(flatNavAbility - (numTurns * goalDist)))\n",
        "    #lavaP = pm.Deterministic(\"lavaP\", SimplePrMargin(lavaAbility, lavaPresence))\n",
        "\n",
        "    #rampP = pm.Deterministic(\"rampP\", SimplePrMargin(rampAbility, rampPresence))\n",
        "\n",
        "    #cvchickP = pm.Deterministic(\"cvChickP\", SimplePrMargin(CVChickAbility, CVTest))\n",
        "    #pctbP = pm.Deterministic(\"pctbP\", SimplePrMargin(PCTBAbility, PCTBTest))\n",
        "\n",
        "    #navP = pm.Deterministic(\"navP\", (flatNavP * lavaP * rampP * rightP * aheadP * leftP * cvchickP * pctbP)) #, visualAcuityP))\n",
        "\n",
        "    #OPLOM = objPermanenceLOMargin(objPermAbility, opTest, CVTest, PCTBTest, lightsOutPresence, goalDist, numChoices, maxPermAbility, lightsOutPenalisation=(uniformAbilitySlack-0.35), uniformAbilitySlack=uniformAbilitySlack)\n",
        "    #OPLOM = objPermanenceLOMargin(objPermAbility, opTest, lightsOutPresence, goalDist, numChoices, maxPermAbility, lightsOutPenalisation=(uniformAbilitySlack-0.35), uniformAbilitySlack=uniformAbilitySlack)\n",
        "    #OPLOM = objPermanenceLOMargin(objPermAbility, opTest, cvchickP, pctbP, lightsOutPresence, goalDist, numChoices, maxPermAbility, lightsOutPenalisation=(uniformAbilitySlack-0.35), uniformAbilitySlack=uniformAbilitySlack)\n",
        "\n",
        "    #OPPerformance = (objPermAbility  - ((goalDist * numChoices)*opTest)) #this returns a value that is lower when (a) ability is lower, (b) the goal is occluded for longer or there are more positions where it could be occluded, or (c) when the lights go out (by a small penalisation)\n",
        "    #OPPerformance = (objPermAbility  - ((goalDist * numChoices)))\n",
        "    #OPPerformance = (objPermAbility  - numChoices)\n",
        "    OPPerformance = (objPermAbility  - ((goalDist*opTest) * (numChoices*opTest)))\n",
        "    #OPability = maxPermAbility - ((maxPermAbility-OPPerformance)*opTest)\n",
        "    #objPermP = pm.Deterministic(\"objPermP\", logistic(OPLOM))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic(OPPerformance))\n",
        "\n",
        "\n",
        "    #flatNavP = pm.Deterministic(\"flatNavP\", logistic((flatNavAbility - (numTurns * (goalDist * (1-opTest))))* (opTest*objPermP)))\n",
        "    #noise = 1 - np.mean(results)  # With this noise is complementary to result prior.\n",
        "    #noisePar = pm.Uniform(\"noisePar\", 0, 1)\n",
        "    #finalP = pm.Deterministic(\"finalP\", (1-noisePar)*(objPermP * navP * visualAcuityP)+(noisePar*noise))\n",
        "\n",
        "    #finalP = pm.Deterministic(\"finalP\", (objPermP * flatNavP))\n",
        "    #finalP = pm.Deterministic(\"finalP\", (1-((1-objPermP)*(1-flatNavP)))) #compensatory finalPerformance\n",
        "\n",
        "    #taskPerformance = pm.Bernoulli(\"taskPerformance\", flatNavP, observed=results)\n",
        "    #taskPerformance = pm.Bernoulli(\"taskPerformance\", finalP, observed=results)\n",
        "    taskPerformance = pm.Bernoulli(\"taskPerformance\", objPermP, observed=results)\n",
        "  pm.model_graph.model_to_graphviz(m)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "edIxFbqFq9V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, min, max = SetupModelSimple(synthetic_agents_data, uniformAbilitySlack=1.4, agent_type='agent', agent_name=\"perfectAgent\", sample = None)\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv.format=\"png\"\n",
        "gv.render\n",
        "#gv.render(directory='viz')\n",
        "#Image(\"viz/Digraph.gv.png\")\n",
        "gv"
      ],
      "metadata": {
        "id": "j-cgtM4msvph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A function for pulling out the means and standard deviations for abilities of interest\n",
        "def analyzeAgentResults(agentName, agentData, abilitiesToShow):\n",
        "\n",
        "  abilityMeans = [] # empty list to add in ability means to\n",
        "  abilitySDs = []\n",
        "  for a in abilitiesToShow: #iterate through each ability, add posterior mean to dataframe, and plot posterior\n",
        "\n",
        "    posteriorMean = float(np.mean(agentData['posterior'][a])) # calculate posterior mean\n",
        "    posteriorSD = float(np.std(agentData['posterior'][a])) #calculate posterior sd\n",
        "    abilityMeans.append(posteriorMean)\n",
        "    abilitySDs.append(posteriorSD)\n",
        "\n",
        "  return abilityMeans, abilitySDs\n",
        "\n",
        "# A function for padding the testing data with 0s for prediction\n",
        "def pad(testingData, trainingDataSize):\n",
        "    return testingData.append(pd.Series(np.zeros(trainingDataSize-len(testingData), dtype=int)))\n",
        "\n",
        "# A function for making predictions based on a fitted measurement layout\n",
        "def predict(m, agentData, dfTest, agent, len_training):\n",
        "  with m:\n",
        "    # set the data for prediction\n",
        "    pm.set_data({\"lavaPresence\": pad(dfTest[\"lavaPresence\"], len_training)})\n",
        "    pm.set_data({\"lightsOutPresence\": pad(dfTest[\"lightsOutPresence\"], len_training)})\n",
        "    pm.set_data({\"rampPresence\": pad(dfTest[\"taskCriticalRampPresence\"], len_training)})\n",
        "    pm.set_data({\"numChoices\": pad(dfTest[\"numChoices\"], len_training)})\n",
        "    pm.set_data({\"minTurnsToGoal\": pad(dfTest[\"minNumTurnsRequired\"], len_training)})\n",
        "    pm.set_data({\"goalDistance\": pad(dfTest[\"cityBlockDistanceToGoal\"], len_training)})\n",
        "    pm.set_data({\"goalSize\": pad(dfTest[\"mainGoalSize\"], len_training)})\n",
        "    pm.set_data({\"goalRight\": pad(dfTest[\"goalRightRelToStart\"], len_training)})\n",
        "    pm.set_data({\"goalAhead\": pad(dfTest[\"goalCentreRelToStart\"], len_training)})\n",
        "    pm.set_data({\"goalLeft\": pad(dfTest[\"goalLeftRelToStart\"], len_training)})\n",
        "    pm.set_data({\"CVChickTest\": pad(dfTest[\"cvchickTask\"], len_training)})\n",
        "    pm.set_data({\"PCTBTest\": pad(dfTest[\"pctbTask\"], len_training)})\n",
        "    pm.set_data({\"allocentricOPTest\": pad(dfTest[\"goalBecomesAllocentricallyOccluded\"], len_training)})\n",
        "\n",
        "    predictions=pm.sample_posterior_predictive(agentData, var_names=[\"finalP\"], return_inferencedata=False,predictions=True,extend_inferencedata=False)\n",
        "    predictionChainRuns =predictions[\"finalP\"][:,:,0:len(dfTest)]\n",
        "    predictionsInstance = np.mean(predictionChainRuns, (0,1))\n",
        "\n",
        "    return predictionsInstance,  dfTest[agent].to_numpy()\n",
        "\n",
        "\n",
        "def brierScore(preds, outs):\n",
        "    return 1/len(preds) * sum( (preds-outs)**2 )\n",
        "\n",
        "def brierDecomp(preds, outs):\n",
        "\n",
        "  brier= 1/len(preds) * sum( (preds-outs)**2 )\n",
        "  ## bin predictions\n",
        "  bins = np.linspace(0,1,11)\n",
        "  binCenters = (bins[:-1] +bins[1:]) /2\n",
        "  binPredInds = np.digitize(preds,binCenters)\n",
        "  binnedPreds = bins[binPredInds]\n",
        "\n",
        "  binTrueFreqs = np.zeros(10)\n",
        "  binPredFreqs = np.zeros(10)\n",
        "  binCounts = np.zeros(10)\n",
        "\n",
        "  for i in range(10):\n",
        "      idx = (preds >= bins[i]) & (preds < bins[i+1])\n",
        "\n",
        "      binTrueFreqs[i] = np.sum(outs[idx])/np.sum(idx) if np.sum(idx) > 0 else 0\n",
        "     # print(np.sum(outs[idx]), np.sum(idx), binTrueFreqs[i])\n",
        "      binPredFreqs[i] = np.mean(preds[idx]) if np.sum(idx) > 0 else 0\n",
        "      binCounts[i] = np.sum(idx)\n",
        "\n",
        "  calibration = np.sum(binCounts * (binTrueFreqs - binPredFreqs) ** 2) / np.sum(binCounts) if np.sum(binCounts)>0 else 0\n",
        "  refinement = np.sum(binCounts * (binTrueFreqs *(1 - binTrueFreqs))) / np.sum(binCounts) if np.sum(binCounts)> 0 else 0\n",
        "  # Compute refinement component\n",
        "  #refinement = brier - calibration\n",
        "  return brier, calibration, refinement\n",
        "\n"
      ],
      "metadata": {
        "id": "-Rm-t0AkyvM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agents_training, agents_test = train_test_split(agent_data, test_size=0.2)\n",
        "children_training, children_test = train_test_split(children_data, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "PmzVmxb1Wpm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, min, max = SetupModel(children_training, uniformAbilitySlack=1.4, agent_type='child', sample = None)"
      ],
      "metadata": {
        "id": "yFLh92O_et4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with m:\n",
        "    childData = pm.sample(500, target_accept=0.95) #Might need more samples to converge"
      ],
      "metadata": {
        "id": "w9aFo8X8eyEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abilitiesToShow = [\"objPermAbility\", \"visualAcuityAbility\", \"flatNavAbility\", \"lavaAbility\", \"rampAbility\", \"rightAbility\", \"leftAbility\", \"aheadAbility\", \"CVChickAbility\", \"PCTBAbility\", \"noisePar\"]\n",
        "\n",
        "m, min, max = SetupModel(children_test, uniformAbilitySlack=1.4, agent_type='child', sample = None)\n",
        "\n",
        "mu, sd  = analyzeAgentResults(\"children\", childData, abilitiesToShow)\n",
        "print(mu)\n",
        "print(sd)\n",
        "#predictions, testOutputs = predict(m, childData, children_test, \"success\", len(children_training[\"success\"]))"
      ],
      "metadata": {
        "id": "SXP4tpmNQ15u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, min, max = SetupModelSimple(synthetic_agents_data, uniformAbilitySlack=1.2, agent_type='agent', agent_name=\"perfectAgent\", sample = None)"
      ],
      "metadata": {
        "id": "5RVVI1yJQOOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with m:\n",
        "    agentData = pm.sample(2000, target_accept=0.95) #Might need more samples to converge"
      ],
      "metadata": {
        "id": "ll_36eHQQqxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#abilitiesToShow = [\"objPermAbility\", \"flatNavAbility\"]\n",
        "abilitiesToShow = [\"objPermAbility\"]\n",
        "mu, sd  = analyzeAgentResults(\"perfectAgent\", agentData, abilitiesToShow)\n",
        "mean_success = np.mean(synthetic_agents_data['perfectAgent'])\n",
        "print(mean_success)\n",
        "print(mu)\n",
        "print(sd)\n",
        "print(max)"
      ],
      "metadata": {
        "id": "9zbaA1yUt2su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentData[\"posterior\"][\"objPermPDet\"]"
      ],
      "metadata": {
        "id": "_Od-asFGIYkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1) = plt.subplots(1,1)\n",
        "az.plot_posterior(agentData[\"posterior\"][\"objPermAbility\"], ax=ax1);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3L9tz7dNGsg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abilitiesToShow = [\"objPermAbility\", \"visualAcuityAbility\", \"flatNavAbility\", \"lavaAbility\", \"rampAbility\", \"rightAbility\", \"leftAbility\", \"aheadAbility\", \"CVChickAbility\", \"PCTBAbility\", \"noisePar\"]\n",
        "mu, sd  = analyzeAgentResults(\"perfectAgent\", agentData, abilitiesToShow)\n",
        "print(mu)\n",
        "print(sd)\n",
        "print(max)"
      ],
      "metadata": {
        "id": "2BLM0Teyp8mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentName = \"Vanilla_Braitenberg_15_rays_over_60_degs_3761\"\n",
        "abilitiesToShow = [\"objPermAbility\", \"visualAcuityAbility\", \"flatNavAbility\", \"lavaAbility\", \"rampAbility\", \"rightAbility\", \"leftAbility\", \"aheadAbility\", \"CVChickAbility\", \"PCTBAbility\", \"noisePar\"]\n",
        "\n",
        "m, min, max = SetupModel(agents_test, uniformAbilitySlack=1.4, agent_type='agent', agent_name=agentName)\n",
        "\n",
        "mu, sd  = analyzeAgentResults(agentName, agentData, abilitiesToShow)\n",
        "print(mu)\n",
        "print(sd)\n",
        "#predictions, testOutputs = predict(m, agentData, agents_test, agentName, len(agents_training[agentName]))"
      ],
      "metadata": {
        "id": "J5xV1fmxb8vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "abilityDF = pd.DataFrame( {'Ability': abilitiesIncludingSuccess})\n",
        "meansDF = pd.DataFrame({\"Ability\": abilitiesIncludingSuccess})\n",
        "stdDevDF = pd.DataFrame({\"Ability\": abilitiesIncludingSuccess})\n",
        "modelBrierScores = []\n",
        "aggregateBrierScores = []\n",
        "modelCalibrations = []\n",
        "aggregateCalibrations = []\n",
        "modelRefinements = []\n",
        "aggregateRefinements = []\n",
        "\n",
        "agentBrierScore, agentCalibration, agentRefinement = brierDecomp(predictions, testOutputs)\n",
        "agentAggBrierScore, agentAggCalibration, agentAggRefinement = brierDecomp(np.repeat(np.mean(agent_data[agentName]), len(testOutputs)), testOutputs)\n",
        "aggregateBrierScores.append(agentAggBrierScore)\n",
        "aggregateCalibrations.append(agentAggCalibration)\n",
        "aggregateRefinements.append(agentAggRefinement)\n",
        "modelBrierScores.append(agentBrierScore)\n",
        "modelCalibrations.append(agentCalibration)\n",
        "modelRefinements.append(agentRefinement)\n",
        "abilityMeans= [str(round(mu_i, 2))+\" \" for mu_i in mu]  # add ability mean estimate to list for this agent\n",
        "abilitySDs = [\"\"+str(round(sd_i,2)) for sd_i in sd ]"
      ],
      "metadata": {
        "id": "-mqyHbZ8jtia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agentBrierScore, agentCalibration, agentRefinement = brierDecomp(predictions, testOutputs)\n",
        "abilityMeans= [str(round(mu_i, 2))+\" \" for mu_i in mu]  # add ability mean estimate to list for this agent\n",
        "abilitySDs = [\"\"+str(round(sd_i,2)) for sd_i in sd ]\n",
        "print(agentBrierScore)\n",
        "print(agentCalibration)\n",
        "print(agentRefinement)\n",
        "print(abilityMeans)\n",
        "print(abilitySDs)"
      ],
      "metadata": {
        "id": "swLWucBTktOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brier= 1/len(predictions) * sum( (predictions-testOutputs)**2 )\n",
        "print(sum( (predictions-testOutputs)**2))\n",
        "# ## bin predictions\n",
        "# bins = np.linspace(0,1,11)\n",
        "# binCenters = (bins[:-1] +bins[1:]) /2\n",
        "# binPredInds = np.digitize(preds,binCenters)\n",
        "# binnedPreds = bins[binPredInds]\n",
        "# binTrueFreqs = np.zeros(10)\n",
        "# binPredFreqs = np.zeros(10)\n",
        "# binCounts = np.zeros(10)\n",
        "# for i in range(10):\n",
        "#   idx = (preds >= bins[i]) & (preds < bins[i+1])\n",
        "\n",
        "#   binTrueFreqs[i] = np.sum(outs[idx])/np.sum(idx) if np.sum(idx) > 0 else 0\n",
        "#   # print(np.sum(outs[idx]), np.sum(idx), binTrueFreqs[i])\n",
        "#   binPredFreqs[i] = np.mean(preds[idx]) if np.sum(idx) > 0 else 0\n",
        "#   binCounts[i] = np.sum(idx)\n",
        "\n",
        "# calibration = np.sum(binCounts * (binTrueFreqs - binPredFreqs) ** 2) / np.sum(binCounts) if np.sum(binCounts)>0 else 0\n",
        "# refinement = np.sum(binCounts * (binTrueFreqs *(1 - binTrueFreqs))) / np.sum(binCounts) if np.sum(binCounts)> 0 else 0\n",
        "# # Compute refinement component\n",
        "# #refinement = brier - calibration\n",
        "\n"
      ],
      "metadata": {
        "id": "9Q0R-aKC0Feo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brierDF = pd.DataFrame({\"Model Brier Score\":modelBrierScores, \"Aggregate Brier Scores\": aggregateBrierScores, \"Model Calibration\": modelCalibrations, \"Aggregate Calibration\":aggregateCalibrations, \"Model Refinement\":modelRefinements, \"Aggregate Refinement\":aggregateRefinements,  \"Model Better? (Based on Brier Score)\":np.array(modelBrierScores)<np.array(aggregateBrierScores), \"Success Score\":meansDF.iloc[9][1:]})"
      ],
      "metadata": {
        "id": "NMsnjVhVdoTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taskResults = agent_data[agentName] #Grab the column of results for that agent\n",
        "print(agent_data)\n",
        "print(np.mean(taskResults)) #print mean performance for the agent across all tasks\n",
        "m = SetupModel(taskResults) #Define the model using the setupModel function. Needs to be redefined each run after taskResults are updated or PyMC won't use the latest taskResults\n",
        "\n",
        "mu, sd  = analyzeAgentResults(agentName, agentData)\n",
        "predictions, testOutputs = predict(m, agentData, agents_test[agentName], len(agents_test[agentName]))\n",
        "agentBrierScore, agentCalibration, agentRefinement = brierDecomp(predictions, testOutputs)\n",
        "agentAggBrierScore, agentAggCalibration, agentAggRefinement = brierDecomp(np.repeat(np.mean(taskResults), len(testOutputs)), testOutputs)\n",
        "aggregateBrierScores.append(agentAggBrierScore)\n",
        "aggregateCalibrations.append(agentAggCalibration)\n",
        "aggregateRefinements.append(agentAggRefinement)\n",
        "modelBrierScores.append(agentBrierScore)\n",
        "modelCalibrations.append(agentCalibration)\n",
        "modelRefinements.append(agentRefinement)\n",
        "abilityMeans= [str(round(mu_i, 2))+\" \" for mu_i in mu]  # add ability mean estimate to list for this agent\n",
        "abilitySDs = [\"\"+str(round(sd_i,2)) for sd_i in sd ]"
      ],
      "metadata": {
        "id": "T0Z1xKhpa2Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu"
      ],
      "metadata": {
        "id": "_5bEjgpv8FIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd"
      ],
      "metadata": {
        "id": "RqzPIRTI8J7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max"
      ],
      "metadata": {
        "id": "7wsEMrD59wNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Sample results for each agent using the model defined in the function above.\n",
        "allAgentData = [] #blank list for entering the outputs of the pm sampling for each agent\n",
        "agentNames = [] #blank list for the agent names\n",
        "agentColList = agentCols.tolist()\n",
        "print(agentColList)\n",
        "numAgents = len(agentColList) #Number of agents, replace with a small number for testing to save time\n",
        "print(numAgents)\n",
        "#numAgents=3\n",
        "abilitiesToShow = [\"objPermAbility\", \"flatNavAbility\", \"visualAcuityAbility\",\"lavaAbility\", \"platformAbility\", \"rampAbility\", \"memoryAbility\", \"rightLeftBias\", \"noisePar\"]\n",
        "\n",
        "abilitiesIncludingSuccess = abilitiesToShow + [\"Success\"]\n",
        "abilityDF = pd.DataFrame( {'Ability': abilitiesIncludingSuccess})\n",
        "meansDF = pd.DataFrame({\"Ability\": abilitiesIncludingSuccess})\n",
        "stdDevDF = pd.DataFrame({\"Ability\": abilitiesIncludingSuccess})\n",
        "modelBrierScores = []\n",
        "aggregateBrierScores = []\n",
        "modelCalibrations = []\n",
        "aggregateCalibrations = []\n",
        "modelRefinements = []\n",
        "aggregateRefinements = []\n",
        "\n",
        "\n",
        "for i in range(numAgents): # Iterate through each agent.\n",
        "  agent = agentCols[i] #Get the agent name\n",
        "  taskResults = df[agent] #Grab the column of results for that agent\n",
        "  print(agent)\n",
        "  print(np.mean(taskResults)) #print mean performance for the agent across all tasks\n",
        "  m = SetupModel(taskResults) #Define the model using the setupModel function. Needs to be redefined each run after taskResults are updated or PyMC won't use the latest taskResults\n",
        "\n",
        "  #Now sample based on this agent's performance\n",
        "  with m:\n",
        "    agentData = pm.sample(1000, target_accept=0.95) #Might need more samples to converge\n",
        "\n",
        "  mu, sd  = analyzeAgentResults(agent, agentData)\n",
        "  predictions, testOutputs = predict(m, agentData, dfTest, agent)\n",
        "  agentBrierScore, agentCalibration, agentRefinement = brierDecomp(predictions, testOutputs)\n",
        "  agentAggBrierScore, agentAggCalibration, agentAggRefinement = brierDecomp(np.repeat(np.mean(taskResults), len(testOutputs)), testOutputs)\n",
        "  aggregateBrierScores.append(agentAggBrierScore)\n",
        "  aggregateCalibrations.append(agentAggCalibration)\n",
        "  aggregateRefinements.append(agentAggRefinement)\n",
        "  modelBrierScores.append(agentBrierScore)\n",
        "  modelCalibrations.append(agentCalibration)\n",
        "  modelRefinements.append(agentRefinement)\n",
        "  abilityMeans= [str(round(mu_i, 2))+\" \" for mu_i in mu]  # add ability mean estimate to list for this agent\n",
        "  abilitySDs = [\"\"+str(round(sd_i,2)) for sd_i in sd ]\n",
        "\n",
        "\n",
        "  abilityMeans = abilityMeans + [str(round(np.mean(taskResults),2))+\" \"]\n",
        "  abilitySDs = abilitySDs + [str(round(np.std(taskResults), 2))]\n",
        "  abilityDF[agent] = list(zip(abilityMeans, abilitySDs))\n",
        "  mu=mu+[np.mean(taskResults)]\n",
        "  sd=sd+[np.std(taskResults)]\n",
        "  meansDF[agent]=[round(mu_i,2) for mu_i in mu]\n",
        "  stdDevDF[agent]=[round(sd_i,2) for sd_i in sd]"
      ],
      "metadata": {
        "id": "PTu7cUuuagVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}